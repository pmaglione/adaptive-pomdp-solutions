# This file is a POMDP policy, represented as a set of "lower bound
# planes", each of which consists of an alpha vector and a corresponding
# action.  Given a particular belief b, this information can be used to
# answer two queries of interest:
#
#   1. What is a lower bound on the expected long-term reward starting
#        from belief b?
#   2. What is an action that achieves that expected reward lower bound?
#
# Each lower bound plane is only defined over a subset of the belief
# simplex--it is defined for those beliefs b such that the non-zero
# entries of b are a subset of the entries present in the plane's alpha
# vector.  If this condition holds we say the plane is 'applicable' to b.
#
# Given a belief b, both of the queries above can be answered by the
# following process: first, throw out all the planes that are not
# applicable to b.  Then, for each of the remaining planes, take the inner
# product of the plane's alpha vector with b.  The highest inner product
# value is the expected long-term reward lower bound, and the action label
# for that plane is the action that achieves the bound.

{
  policyType => "MaxPlanesLowerBound",
  numPlanes => 13,
  planes => [
    {
      action => 1,
      numEntries => 7,
      entries => [
        0, 0,
        1, 0,
        2, 0,
        3, 0,
        5, -10,
        6, -10,
        7, -10
      ]
    },
    {
      action => 2,
      numEntries => 9,
      entries => [
        0, -500,
        1, -500,
        2, -500,
        3, -500,
        4, 0,
        5, 0,
        6, 0,
        7, 0,
        8, 0
      ]
    },
    {
      action => 2,
      numEntries => 7,
      entries => [
        1, -500,
        2, -500,
        3, -500,
        4, 0,
        5, 0,
        6, 0,
        7, 0
      ]
    },
    {
      action => 0,
      numEntries => 6,
      entries => [
        1, -86.3619,
        2, -178.982,
        3, -283.087,
        5, -2.10694,
        6, -4.00597,
        7, -6.24874
      ]
    },
    {
      action => 0,
      numEntries => 6,
      entries => [
        1, -15.2483,
        2, -60.058,
        3, -142.53,
        5, -4.40895,
        6, -6.9834,
        7, -9.12356
      ]
    },
    {
      action => 0,
      numEntries => 6,
      entries => [
        1, -3.51571,
        2, -20.8172,
        3, -72.2576,
        5, -6.33094,
        6, -8.97808,
        7, -10.5608
      ]
    },
    {
      action => 0,
      numEntries => 6,
      entries => [
        1, -3.43759,
        2, -19.402,
        3, -64.2312,
        5, -6.70939,
        6, -9.1099,
        7, -10.4985
      ]
    },
    {
      action => 0,
      numEntries => 7,
      entries => [
        1, -83.4917,
        2, -165.983,
        3, -250.975,
        4, -1,
        5, -2.64984,
        6, -4.29967,
        7, -5.9995
      ]
    },
    {
      action => 0,
      numEntries => 7,
      entries => [
        1, -14.7748,
        2, -55.7691,
        3, -126.475,
        4, -1.9999,
        5, -4.86223,
        6, -7.18016,
        7, -8.99895
      ]
    },
    {
      action => 0,
      numEntries => 7,
      entries => [
        1, -3.43759,
        2, -19.402,
        3, -64.2312,
        4, -2.9997,
        5, -6.70939,
        6, -9.1099,
        7, -10.4985
      ]
    },
    {
      action => 0,
      numEntries => 7,
      entries => [
        1, -1.56715,
        2, -7.402,
        3, -33.1124,
        4, -3.9994,
        5, -8.25161,
        6, -10.4027,
        7, -11.2482
      ]
    },
    {
      action => 0,
      numEntries => 7,
      entries => [
        1, -1.25855,
        2, -3.44242,
        3, -17.5545,
        4, -4.999,
        5, -9.53924,
        6, -11.2688,
        7, -11.6231
      ]
    },
    {
      action => 1,
      numEntries => 9,
      entries => [
        0, 0,
        1, 0,
        2, 0,
        3, 0,
        4, -10,
        5, -10,
        6, -10,
        7, -10,
        8, 0
      ]
    }
  ]
}
