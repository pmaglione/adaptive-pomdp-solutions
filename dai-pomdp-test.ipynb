{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POMDP APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dai_pomdp.ModelLearning.utils import *\n",
    "from dai_pomdp.Data import *\n",
    "from dai_pomdp.Ballots import *\n",
    "from dai_pomdp.helpers import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import subprocess\n",
    "#from random import random\n",
    "from os import mkdir, rmdir\n",
    "from copy import deepcopy\n",
    "from math import floor\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ACTIONS\n",
    "CONST_NO_ANSWER = -1\n",
    "CONST_REQUEST_VOTE = 0\n",
    "CONST_SUBMIT_ZERO = 1\n",
    "CONST_SUBMIT_ONE = 2\n",
    "\n",
    "def round_to_3(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "def are_items_unresolved(answers):\n",
    "    return any(answer == CONST_NO_ANSWER for answer in answers)\n",
    "\n",
    "def get_unresolved_items(answers):\n",
    "    return [key for key, answer in enumerate(answers) if answer == CONST_NO_ANSWER]\n",
    "\n",
    "def get_random_worker(workers_error_rates, item_id, votes):\n",
    "    item_votes = votes[item_id].copy()\n",
    "    worker_ids_used = item_votes.keys()\n",
    "    workers_ids_range = [k for k,v in enumerate(workers_error_rates)]\n",
    "    workers_ids_unused = [val for val in workers_ids_range if val not in worker_ids_used]\n",
    "    \n",
    "    if (len(workers_ids_unused) == 0):\n",
    "        used = len(worker_ids_used)\n",
    "        ranges = len(workers_ids_range)\n",
    "        unu = len(workers_ids_unused)\n",
    "        print(f'used: {used}')\n",
    "        print(f'workers: {ranges}')\n",
    "        print(f'unused: {unu}')\n",
    "        raise ValueError(\"Unused empty!?\")\n",
    "    \n",
    "    selected_worker_id = np.random.choice(workers_ids_unused)\n",
    "    error_rate = workers_error_rates[selected_worker_id]\n",
    "\n",
    "    return selected_worker_id, error_rate\n",
    "\n",
    "def get_accuracy(item_diff, worker_error_rate):\n",
    "    return (1.0 / 2) * (1.0 + (1.0 - item_diff) ** worker_error_rate)\n",
    "\n",
    "def get_worker_vote(item_id, items_votes, items_difficulties, items_gt, workers_error_rates):\n",
    "    selected_worker_id, worker_error_rate = get_random_worker(workers_error_rates, item_id, items_votes)\n",
    "    worker_acc = get_accuracy(items_difficulties[item_id], worker_error_rate)\n",
    "    \n",
    "    if np.random.binomial(1, worker_acc):\n",
    "        return selected_worker_id, items_gt[item_id]\n",
    "    else:\n",
    "        return selected_worker_id, 1 - items_gt[item_id]\n",
    "    \n",
    "def get_worker_error_rate_estimation(items_votes):\n",
    "    # min 2 votes per item\n",
    "    if all(len(v) >= 2 for k, v in items_votes.items()):\n",
    "        workers_to_int = writeToEMFormat(items_votes)\n",
    "\n",
    "        gammas, difficulties, posteriors = get_results()\n",
    "\n",
    "        worker_keys = list(workers_to_int.keys())\n",
    "        worker_int_keys = list(workers_to_int.values())\n",
    "\n",
    "        return {worker_keys[worker_int_keys.index(key)]: gamma for key, gamma in enumerate(gammas)}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_error_rate(worker_id, estimated_error_rates, avg_error_rate):\n",
    "    if worker_id in estimated_error_rates.keys():\n",
    "        return estimated_error_rates[worker_id]\n",
    "    elif len(estimated_error_rates) != 0:\n",
    "        return sum(estimated_error_rates.values()) / len(estimated_error_rates)  # AVG over known workers\n",
    "    else:\n",
    "        return avg_error_rate\n",
    "\n",
    "\n",
    "def get_worker_error_rate(worker_id, estimated_error_rates, avg_error_rate, estimate_after, have_submitted):\n",
    "    if estimate_after:\n",
    "        if have_submitted:\n",
    "            return get_error_rate(worker_id, estimated_error_rates, avg_error_rate)\n",
    "        else:\n",
    "            return avg_error_rate\n",
    "    else:\n",
    "        return get_error_rate(worker_id, estimated_error_rates, avg_error_rate)\n",
    "\n",
    "#data utils\n",
    "'''\n",
    "    items_num - number of items\n",
    "    possitive_percentage - [0,1] percentage of possitive items\n",
    "'''\n",
    "def generate_gold_data(items_num, possitive_percentage):\n",
    "    pos_items_number = int(round(((possitive_percentage * 100) * items_num) / 100))     \n",
    "    gold_data = ([1] * pos_items_number) + ([0] * (items_num - pos_items_number))\n",
    "    random.shuffle(gold_data)\n",
    "\n",
    "    return gold_data\n",
    "\n",
    "def solve(num_states, states_difficulties, avg_error_rate, policy, workers_error_rates, items_difficulties, items_gt , estimate_after = True):\n",
    "    \n",
    "    num_items = len(items_gt)\n",
    "    \n",
    "    actions = range(0, 3) # 0,1,2\n",
    "    #states_difficulties = getDifficulties(0.1)\n",
    "\n",
    "    items_votes = {}\n",
    "    for item_id in range(num_items):\n",
    "        items_votes[item_id] = {}\n",
    "\n",
    "    # init beliefs\n",
    "    belief = [1 for i in range(num_states)]\n",
    "    belief[num_states-1] = 0  # last states = 0, terminating state\n",
    "    belief = normalize(belief)\n",
    "    beliefs = [deepcopy(belief) for i in range(num_items)]\n",
    "    \n",
    "    answers = [CONST_NO_ANSWER for i in range(0, num_items)]\n",
    "    \n",
    "    iteration_number = 0\n",
    "    while are_items_unresolved(answers):\n",
    "        iteration_number += 1\n",
    "        \n",
    "        items_to_vote = []\n",
    "        unresolved_items = get_unresolved_items(answers)\n",
    "        unresolved_items_num = len(unresolved_items)\n",
    "        \n",
    "        for item_id in unresolved_items:\n",
    "            beliefState = beliefs[item_id]\n",
    "            bestAction = findBestAction(actions, policy, beliefState)\n",
    "            bestAction = int(bestAction)\n",
    "            \n",
    "            if bestAction == CONST_REQUEST_VOTE:\n",
    "                items_to_vote.append(item_id)\n",
    "            elif bestAction == CONST_SUBMIT_ZERO or bestAction == CONST_SUBMIT_ONE:\n",
    "                if bestAction == CONST_SUBMIT_ZERO:\n",
    "                    answers[item_id] = 0\n",
    "                else:\n",
    "                    answers[item_id] = 1\n",
    "\n",
    "        #end for\n",
    "        \n",
    "        have_submitted = unresolved_items_num != num_items\n",
    "        \n",
    "        for item_to_vote in items_to_vote:\n",
    "            worker_id, vote = get_worker_vote(item_to_vote, items_votes, items_difficulties, items_gt, workers_error_rates)\n",
    "            items_votes[item_to_vote][worker_id] = vote\n",
    "        \n",
    "        \n",
    "        estimated_error_rates = get_worker_error_rate_estimation(items_votes)\n",
    "        \n",
    "        for item_id in items_to_vote:\n",
    "            last_vote = list(items_votes[item_id].values())[-1]\n",
    "            last_worker_id = list(items_votes[item_id])[-1]\n",
    "            beliefs[item_id] = updateBelief(beliefs[item_id], last_vote, states_difficulties,\n",
    "                                     get_worker_error_rate(last_worker_id, estimated_error_rates, avg_error_rate, estimate_after, have_submitted))\n",
    "            \n",
    "        #print(f\"Num to vote: {len(items_to_vote)}\")\n",
    "    #end while\n",
    "            \n",
    "    return answers, items_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genPOMDP(filename, fnc, fpc, cost_vote, gammas, numberOfWorkerPools, difficulties=None):\n",
    "\n",
    "    if difficulties is None:\n",
    "        difficulties = getDifficulties(0.1)\n",
    "\n",
    "    numDiffs = len(difficulties)\n",
    "\n",
    "    reward_correct_answer = 0\n",
    "    \n",
    "    #Add one absorbing state\n",
    "    numberOfStates = ((numDiffs) * 2) + 1\n",
    "    numberOfActions = numberOfWorkerPools + 2\n",
    "    file = open(filename, 'w')\n",
    "    file.write('discount: 0.9999\\n')\n",
    "    file.write('values: reward\\n')\n",
    "    file.write('states: %d\\n' % numberOfStates)\n",
    "    file.write('actions: %d\\n' % numberOfActions)\n",
    "    SUBMITZERO = numberOfWorkerPools  # 2 worker pools\n",
    "    SUBMITONE = numberOfWorkerPools + 1  # 3 , for 2 worker pools\n",
    "    file.write('observations: Zero One None\\n')\n",
    "\n",
    "    for i in range(0, numberOfStates):\n",
    "        for k in range(0, numberOfWorkerPools):\n",
    "            file.write('T: %d : %d : %d %f\\n' % (k, i, i, 1.0))\n",
    "\n",
    "    #Add transitions to absorbing state\n",
    "    file.write('T: %d : * : %d %f\\n' % (SUBMITZERO, numberOfStates-1, 1.0))\n",
    "    file.write('T: %d : * : %d %f\\n' % (SUBMITONE, numberOfStates-1, 1.0))\n",
    "\n",
    "    #Add observations in absorbing state\n",
    "    file.write('O: * : %d : None %f\\n' % (numberOfStates-1, 1.0))\n",
    "\n",
    "    for v in range(0, 2):\n",
    "        for diffState in range(0,numDiffs):\n",
    "        #for diffState in product(range(numDiffs), repeat = numberOfWorkerPools):\n",
    "            state = v * numDiffs + diffState\n",
    "            \"\"\"for k in range(0, numberOfWorkerPools):\n",
    "                state += (diffState[k] * (numDiffs ** (numberOfWorkerPools - (k+1))))\"\"\"\n",
    "            file.write('O: %d: %d : None %f\\n' % (SUBMITZERO, state, 1.0))\n",
    "            file.write('O: %d: %d : None %f\\n' % (SUBMITONE, state, 1.0))\n",
    "            if v == 0: #if the true answer is 0\n",
    "                for k in range(0, numberOfWorkerPools):\n",
    "                    file.write('O: %d : %d : Zero %f\\n' % \n",
    "                               (k, state, calcAccuracy(gammas[k], difficulties[diffState])))\n",
    "                    # gamma: shape * scale. i.e: gamma(4,0.42) = 1.68\n",
    "                    file.write('O: %d : %d : One %f\\n' % \n",
    "                               (k, state, 1.0 - calcAccuracy(gammas[k], difficulties[diffState])))\n",
    "            else: # if the answer is 1\n",
    "                for k in range(0, numberOfWorkerPools):\n",
    "                    file.write('O: %d : %d : Zero %f\\n' % \n",
    "                               (k, state, 1.0 - calcAccuracy(gammas[k], difficulties[diffState])))\n",
    "                    file.write('O: %d : %d : One %f\\n' % \n",
    "                               (k, state, calcAccuracy(gammas[k], difficulties[diffState])))\n",
    "\n",
    "\n",
    "    for v in range(numberOfWorkerPools):\n",
    "        file.write('R: %d : * : * : * %f\\n' % (v, cost_vote))  # reward request more vote\n",
    "\n",
    "\n",
    "    for i in range(numberOfStates-1):\n",
    "        if i < (numberOfStates-1) / 2:  # true label = 0\n",
    "            file.write('R: %d : %d : %d : * %f\\n' % (SUBMITZERO, i, numberOfStates-1, reward_correct_answer))\n",
    "            file.write('R: %d : %d : %d : * %f\\n' % (SUBMITONE, i, numberOfStates-1, fpc))\n",
    "        else:  # true label = 1 \n",
    "            file.write('R: %d : %d : %d : * %f\\n' % (SUBMITONE, i, numberOfStates-1, reward_correct_answer))\n",
    "            file.write('R: %d : %d : %d : * %f\\n' % (SUBMITZERO, i, numberOfStates-1, fnc))\n",
    "\n",
    "    #Add rewards in absorbing state\n",
    "    file.write('R: * : %d : %d : * %f\\n' % (numberOfStates-1, numberOfStates-1, 0))\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base config with diff wrong costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "#wrong_answer_costs = [10,20,50,500,1000]\n",
    "wrong_answer_costs = [50]\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for wrong_cost in wrong_answer_costs:\n",
    "        total_results = []\n",
    "\n",
    "        #policy_name = f\"unclassified{unclassify_val}.policy\"\n",
    "        policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "        policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "            costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "            # end for iterations\n",
    "\n",
    "        result = [f\"base-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "        total_results.append(result)\n",
    "        pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "    #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base config with differents FNC(false negative cost) and FPC(false positive cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for fnc in fncs:\n",
    "        for fpc in fpcs:\n",
    "            total_results = []\n",
    "\n",
    "            policy_name = f'wrong-cost-fnc{fnc}-fpc{fpc}.policy'\n",
    "            policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "            losses = []\n",
    "            recalls = []\n",
    "            precisions = []\n",
    "            costs = []\n",
    "            f_ones = []\n",
    "            f_betas = []\n",
    "            wces = []\n",
    "\n",
    "            for _ in range(10):\n",
    "                answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "                \n",
    "                loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                losses.append(loss)\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f_ones.append(f1)\n",
    "                f_betas.append(f_beta)\n",
    "                wces.append(wce)\n",
    "                # end for iterations\n",
    "\n",
    "            result = [f\"wrong-cost-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                      item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                     round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                     round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                      round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "            total_results.append(result)\n",
    "            pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "        #end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different state numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFraimport algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "states_nums = [5,9,13,17,43,203]\n",
    "states_diffs = [[0,1], #5 states\n",
    "         [0, 0.33, 0.66, 1], #9 states\n",
    "         [0, 0.2, 0.4, 0.6, 0.8, 1], # 13 states\n",
    "         [0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1], #17 states\n",
    "         np.arange(0, 1.05, .05), #43\n",
    "         np.arange(0, 1.01, .01) # 203\n",
    "        ]\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "\n",
    "for ind, states_num in enumerate(states_nums):\n",
    "    for moment_error_estimation in [False, True]:\n",
    "        for wrong_cost in wrong_answer_costs:\n",
    "            total_results = []\n",
    "            \n",
    "            state_diff = states_diffs[ind]\n",
    "\n",
    "            #policy_name = f\"unclassified{unclassify_val}.policy\"\n",
    "            policy_name = f\"{states_num}states-c{wrong_cost}.policy\"\n",
    "            policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "            losses = []\n",
    "            recalls = []\n",
    "            precisions = []\n",
    "            costs = []\n",
    "            f_ones = []\n",
    "            f_betas = []\n",
    "\n",
    "            for _ in range(10):\n",
    "                answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "                loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "                losses.append(loss)\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f_ones.append(f1)\n",
    "                f_betas.append(f_beta)\n",
    "                # end for iterations\n",
    "\n",
    "            result = [f\"vary_num_states-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                      item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                     round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                     round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "            total_results.append(result)\n",
    "            pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "        #end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify fnc and fpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "states_nums = [5,9,13,17,43,203]\n",
    "states_diffs = [[0,1], #5 states\n",
    "         [0, 0.33, 0.66, 1], #9 states\n",
    "         [0, 0.2, 0.4, 0.6, 0.8, 1], # 13 states\n",
    "         [0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1], #17 states\n",
    "         np.arange(0, 1.05, .05), #43\n",
    "         np.arange(0, 1.01, .01) # 203\n",
    "        ]\n",
    "\n",
    "\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "num_pools = 1\n",
    "gamma_media = [1]\n",
    "cost_request_vote = -1\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "for ind, states_num in enumerate(states_nums):\n",
    "    for moment_error_estimation in [False, True]:\n",
    "        for fnc in fncs:\n",
    "            for fpc in fpcs:\n",
    "                total_results = []\n",
    "                \n",
    "                state_diff = states_diffs[ind]\n",
    "\n",
    "                name = f'vc-{states_num}states-fnc{fnc}-fpc{fpc}'\n",
    "                policy_name = f'{name}.policy'\n",
    "                policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "                losses = []\n",
    "                recalls = []\n",
    "                precisions = []\n",
    "                costs = []\n",
    "                f_ones = []\n",
    "                f_betas = []\n",
    "                wces = []\n",
    "\n",
    "                for _ in range(10):\n",
    "                    answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                    costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                    loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                    losses.append(loss)\n",
    "                    recalls.append(recall)\n",
    "                    precisions.append(precision)\n",
    "                    f_ones.append(f1)\n",
    "                    f_betas.append(f_beta)\n",
    "                    wces.append(wce)\n",
    "                    # end for iterations\n",
    "\n",
    "                result = [f\"diff-states-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                          item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                         round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                         round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                          round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "                total_results.append(result)\n",
    "                pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "            #end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bimodal 23 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "#bimodal\n",
    "workers_error_rates1 = np.random.normal(.2, .01, int(workers_num/2))\n",
    "workers_error_rates2 = np.random.normal(4, .2, int(workers_num/2))\n",
    "workers_error_rates = np.concatenate((workers_error_rates1, workers_error_rates2), axis=0)\n",
    "dist_name = \"BiModal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for wrong_cost in wrong_answer_costs:\n",
    "        total_results = []\n",
    "\n",
    "        #policy_name = f\"unclassified{unclassify_val}.policy\"\n",
    "        policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "        policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "            costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "            # end for iterations\n",
    "\n",
    "        result = [f\"bimodal-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "        total_results.append(result)\n",
    "        pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "    #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bimodal with diff fnc and fpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "#bimodal\n",
    "workers_error_rates1 = np.random.normal(.2, .01, int(workers_num/2))\n",
    "workers_error_rates2 = np.random.normal(4, .2, int(workers_num/2))\n",
    "workers_error_rates = np.concatenate((workers_error_rates1, workers_error_rates2), axis=0)\n",
    "dist_name = \"BiModal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for fnc in fncs:\n",
    "            for fpc in fpcs:\n",
    "                total_results = []\n",
    "\n",
    "    \n",
    "                policy_name = f'wrong-cost-fnc{fnc}-fpc{fpc}.policy'\n",
    "                policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "                losses = []\n",
    "                recalls = []\n",
    "                precisions = []\n",
    "                costs = []\n",
    "                f_ones = []\n",
    "                f_betas = []\n",
    "                wces = []\n",
    "\n",
    "                for _ in range(10):\n",
    "                    answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                    costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                    loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                    losses.append(loss)\n",
    "                    recalls.append(recall)\n",
    "                    precisions.append(precision)\n",
    "                    f_ones.append(f1)\n",
    "                    f_betas.append(f_beta)\n",
    "                    wces.append(wce)\n",
    "\n",
    "                result = [f\"bimodal-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                          item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                         round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                         round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                          round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "                total_results.append(result)\n",
    "                pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "            #end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good and bad workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 1000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "#bimodal\n",
    "#workers_error_rates1 = np.random.normal(.2, .01, int(workers_num/2))\n",
    "#workers_error_rates2 = np.random.normal(4, .2, int(workers_num/2))\n",
    "#workers_error_rates = np.concatenate((workers_error_rates1, workers_error_rates2), axis=0)\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(3, .2, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "#policy = readPolicy(\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/s9-500.policy\", states_num)\n",
    "\n",
    "wrong_answer_costs = [10,20,50] #[10,20,50,500,1000]\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [True]:#[False, True]:\n",
    "    for wrong_cost in wrong_answer_costs:\n",
    "        total_results = []\n",
    "\n",
    "        #policy_name = f\"unclassified{unclassify_val}.policy\"\n",
    "        policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "        policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "            costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "            # end for iterations\n",
    "\n",
    "        result = [f\"bad-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "        total_results.append(result)\n",
    "        pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "    #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### good-bad workers diff fnc and fpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 10000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "#good-bad\n",
    "#workers_error_rates = np.random.normal(.2, .01, workers_num) #good\n",
    "workers_error_rates = np.random.normal(4, .2, workers_num) # bad\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for fnc in fncs:\n",
    "            for fpc in fpcs:\n",
    "                total_results = []\n",
    "\n",
    "    \n",
    "                policy_name = f'wrong-cost-fnc{fnc}-fpc{fpc}.policy'\n",
    "                policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "                losses = []\n",
    "                recalls = []\n",
    "                precisions = []\n",
    "                costs = []\n",
    "                f_ones = []\n",
    "                f_betas = []\n",
    "                wces = []\n",
    "\n",
    "                for _ in range(10):\n",
    "                    answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                    costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                    loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                    losses.append(loss)\n",
    "                    recalls.append(recall)\n",
    "                    precisions.append(precision)\n",
    "                    f_ones.append(f1)\n",
    "                    f_betas.append(f_beta)\n",
    "                    wces.append(wce)\n",
    "\n",
    "                result = [f\"bad-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                          item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                         round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                         round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                          round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "                total_results.append(result)\n",
    "                pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "            #end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "\n",
    "balances = [.01, .1, .3]\n",
    "\n",
    "for balance in balances: \n",
    "    possitive_percentage = balance\n",
    "    items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "\n",
    "    for moment_error_estimation in [False, True]:\n",
    "        for wrong_cost in wrong_answer_costs:\n",
    "            total_results = []\n",
    "\n",
    "            policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "            policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "            losses = []\n",
    "            recalls = []\n",
    "            precisions = []\n",
    "            costs = []\n",
    "            f_ones = []\n",
    "            f_betas = []\n",
    "\n",
    "            for _ in range(10):\n",
    "                answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "                loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "                losses.append(loss)\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f_ones.append(f1)\n",
    "                f_betas.append(f_beta)\n",
    "                # end for iterations\n",
    "\n",
    "            result = [f\"unbalanced-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                      item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                     round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                     round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "            total_results.append(result)\n",
    "            pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "        #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbalance with different fnc and fpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 100\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(normal_mean, normal_std, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "\n",
    "balances = [.01, .1, .3]\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "for balance in balances: \n",
    "    possitive_percentage = balance\n",
    "    items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "\n",
    "    for moment_error_estimation in [False, True]:\n",
    "        for fnc in fncs:\n",
    "            for fpc in fpcs:\n",
    "                total_results = []\n",
    "\n",
    "                policy_name = f'wrong-cost-fnc{fnc}-fpc{fpc}.policy'\n",
    "                policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "                losses = []\n",
    "                recalls = []\n",
    "                precisions = []\n",
    "                costs = []\n",
    "                f_ones = []\n",
    "                f_betas = []\n",
    "                wces = []\n",
    "\n",
    "                for _ in range(10):\n",
    "                    answers, items_votes = solve(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                    costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                    loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                    losses.append(loss)\n",
    "                    recalls.append(recall)\n",
    "                    precisions.append(precision)\n",
    "                    f_ones.append(f1)\n",
    "                    f_betas.append(f_beta)\n",
    "                    wces.append(wce)\n",
    "\n",
    "                result = [f\"unbalanced-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                              item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                             round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                             round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                              round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "                total_results.append(result)\n",
    "                pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "            #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassify action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_UNCLASSIFIED = 3\n",
    "\n",
    "def solve_unclassify(num_states, states_difficulties, avg_error_rate, policy, workers_error_rates, items_difficulties, items_gt , estimate_after = True, expert_cost = 20):\n",
    "    \n",
    "    num_items = len(items_gt)\n",
    "    \n",
    "    actions = range(0, 4) # 0,1,2,3\n",
    "    #states_difficulties = getDifficulties(0.1)\n",
    "\n",
    "    items_votes = {}\n",
    "    for item_id in range(num_items):\n",
    "        items_votes[item_id] = {}\n",
    "\n",
    "    # init beliefs\n",
    "    belief = [1 for i in range(num_states)]\n",
    "    belief[num_states-1] = 0  # last states = 0, terminating state\n",
    "    belief = normalize(belief)\n",
    "    beliefs = [deepcopy(belief) for i in range(num_items)]\n",
    "    \n",
    "    answers = [CONST_NO_ANSWER for i in range(0, num_items)]\n",
    "    \n",
    "    iteration_number = 0\n",
    "    while are_items_unresolved(answers):\n",
    "        iteration_number += 1\n",
    "        \n",
    "        items_to_vote = []\n",
    "        unresolved_items = get_unresolved_items(answers)\n",
    "        unresolved_items_num = len(unresolved_items)\n",
    "        \n",
    "        for item_id in unresolved_items:\n",
    "            beliefState = beliefs[item_id]\n",
    "            bestAction = findBestAction(actions, policy, beliefState)\n",
    "            bestAction = int(bestAction)\n",
    "            \n",
    "            if bestAction == CONST_REQUEST_VOTE:\n",
    "                items_to_vote.append(item_id)\n",
    "            elif bestAction == CONST_SUBMIT_ZERO or bestAction == CONST_SUBMIT_ONE or bestAction == CONST_UNCLASSIFIED:\n",
    "                \n",
    "                if bestAction == CONST_UNCLASSIFIED:\n",
    "                    fake_w_id = 1100\n",
    "                    for _ in range(expert_cost):\n",
    "                        items_votes[item_id][fake_w_id] = items_gt[item_id]\n",
    "                    fake_w_id += 1\n",
    "\n",
    "                    answers[item_id] = items_gt[item_id]\n",
    "                elif bestAction == CONST_SUBMIT_ZERO:\n",
    "                    answers[item_id] = 0\n",
    "                else:\n",
    "                    answers[item_id] = 1\n",
    "\n",
    "        #end for\n",
    "        \n",
    "        have_submitted = unresolved_items_num != num_items\n",
    "        \n",
    "        for item_to_vote in items_to_vote:\n",
    "            worker_id, vote = get_worker_vote(item_to_vote, items_votes, items_difficulties, items_gt, workers_error_rates)\n",
    "            items_votes[item_to_vote][worker_id] = vote\n",
    "        \n",
    "        \n",
    "        estimated_error_rates = get_worker_error_rate_estimation(items_votes)\n",
    "        \n",
    "        for item_id in items_to_vote:\n",
    "            last_vote = list(items_votes[item_id].values())[-1]\n",
    "            last_worker_id = list(items_votes[item_id])[-1]\n",
    "            beliefs[item_id] = updateBelief(beliefs[item_id], last_vote, states_difficulties,\n",
    "                                     get_worker_error_rate(last_worker_id, estimated_error_rates, avg_error_rate, estimate_after, have_submitted))\n",
    "            \n",
    "        #print(f\"Num to vote: {len(items_to_vote)}\")\n",
    "    #end while\n",
    "            \n",
    "    return answers, items_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 1000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "#bimodal\n",
    "#workers_error_rates1 = np.random.normal(.2, .01, int(workers_num/2))\n",
    "#workers_error_rates2 = np.random.normal(4, .2, int(workers_num/2))\n",
    "#workers_error_rates = np.concatenate((workers_error_rates1, workers_error_rates2), axis=0)\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(3, .2, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "unclassify_costs = [5,10,20,200,500]\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for ind in range(len(wrong_answer_costs)):\n",
    "        wrong_cost = wrong_answer_costs[ind]\n",
    "        unclassify_cost = unclassify_costs[ind]\n",
    "        \n",
    "        total_results = []\n",
    "\n",
    "        policy_name = f'unclassified-w{wrong_cost}-u{unclassify_cost}.policy'\n",
    "        policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            answers, items_votes = solve_unclassify(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "            costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "        # end for iterations\n",
    "\n",
    "        result = [f\"unclassify-c{wrong_cost}-u{unclassify_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "        total_results.append(result)\n",
    "        pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "    #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassify action with different fnc and fpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 1000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"Normal\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = np.random.normal(1, .2, workers_num)\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for fnc in fncs:\n",
    "        for fpc in fpcs:\n",
    "            \n",
    "            ucc = min(abs(fnc), abs(fpc)) / 2\n",
    "            total_results = []\n",
    "            \n",
    "            policy_name = f'unclassify-fnc{fnc}-fpc{fpc}-u{ucc}.policy'\n",
    "            policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "            losses = []\n",
    "            recalls = []\n",
    "            precisions = []\n",
    "            costs = []\n",
    "            f_ones = []\n",
    "            f_betas = []\n",
    "            wces = []\n",
    "\n",
    "            for _ in range(10):\n",
    "                answers, items_votes = solve_unclassify(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                losses.append(loss)\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f_ones.append(f1)\n",
    "                f_betas.append(f_beta)\n",
    "                wces.append(wce)\n",
    "\n",
    "            # end for iterations\n",
    "\n",
    "            result = [f\"unclassify-fnc{fnc}-fpc{fpc}-u{ucc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                  round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "            \n",
    "            total_results.append(result)\n",
    "            pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "        #end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_vote_confusion(item_id, items_votes, items_difficulties, items_gt, workers_error_rates):\n",
    "    selected_worker_id = get_random_worker_id(workers_error_rates, item_id, items_votes)\n",
    "    \n",
    "    if items_gt[item_id] == 0:\n",
    "        worker_acc = get_accuracy(items_difficulties[item_id], workers_error_rates[0][selected_worker_id])\n",
    "    else:\n",
    "        worker_acc = get_accuracy(items_difficulties[item_id], workers_error_rates[1][selected_worker_id])\n",
    "\n",
    "    if np.random.binomial(1, worker_acc):\n",
    "        return selected_worker_id, items_gt[item_id]\n",
    "    else:\n",
    "        return selected_worker_id, 1 - items_gt[item_id]\n",
    "    \n",
    "def get_random_worker_id(workers_error_rates, item_id, votes):\n",
    "    item_votes = votes[item_id].copy()\n",
    "    worker_ids_used = item_votes.keys()\n",
    "    workers_ids_range = [k for k,v in enumerate(workers_error_rates[0])]\n",
    "    workers_ids_unused = [val for val in workers_ids_range if val not in worker_ids_used]\n",
    "    \n",
    "    if (len(workers_ids_unused) == 0):\n",
    "        used = len(worker_ids_used)\n",
    "        ranges = len(workers_ids_range)\n",
    "        unu = len(workers_ids_unused)\n",
    "        print(f'used: {used}')\n",
    "        print(f'workers: {ranges}')\n",
    "        print(f'unused: {unu}')\n",
    "        raise ValueError(\"Unused empty!?\")\n",
    "    \n",
    "    selected_worker_id = np.random.choice(workers_ids_unused)\n",
    "\n",
    "    return selected_worker_id\n",
    "    \n",
    "def solve_confusion(num_states, states_difficulties, avg_error_rate, policy, workers_error_rates, items_difficulties, items_gt , estimate_after = True):\n",
    "    \n",
    "    num_items = len(items_gt)\n",
    "    \n",
    "    actions = range(0, 3) # 0,1,2\n",
    "    #states_difficulties = getDifficulties(0.1)\n",
    "\n",
    "    items_votes = {}\n",
    "    for item_id in range(num_items):\n",
    "        items_votes[item_id] = {}\n",
    "\n",
    "    # init beliefs\n",
    "    belief = [1 for i in range(num_states)]\n",
    "    belief[num_states-1] = 0  # last states = 0, terminating state\n",
    "    belief = normalize(belief)\n",
    "    beliefs = [deepcopy(belief) for i in range(num_items)]\n",
    "    \n",
    "    answers = [CONST_NO_ANSWER for i in range(0, num_items)]\n",
    "    \n",
    "    iteration_number = 0\n",
    "    while are_items_unresolved(answers):\n",
    "        iteration_number += 1\n",
    "        \n",
    "        items_to_vote = []\n",
    "        unresolved_items = get_unresolved_items(answers)\n",
    "        unresolved_items_num = len(unresolved_items)\n",
    "        \n",
    "        for item_id in unresolved_items:\n",
    "            beliefState = beliefs[item_id]\n",
    "            bestAction = findBestAction(actions, policy, beliefState)\n",
    "            bestAction = int(bestAction)\n",
    "            \n",
    "            if bestAction == CONST_REQUEST_VOTE:\n",
    "                items_to_vote.append(item_id)\n",
    "            elif bestAction == CONST_SUBMIT_ZERO or bestAction == CONST_SUBMIT_ONE:\n",
    "                if bestAction == CONST_SUBMIT_ZERO:\n",
    "                    answers[item_id] = 0\n",
    "                else:\n",
    "                    answers[item_id] = 1\n",
    "\n",
    "        #end for\n",
    "        \n",
    "        have_submitted = unresolved_items_num != num_items\n",
    "        \n",
    "        for item_to_vote in items_to_vote:\n",
    "            worker_id, vote = get_worker_vote_confusion(item_to_vote, items_votes, items_difficulties, items_gt, workers_error_rates)\n",
    "            items_votes[item_to_vote][worker_id] = vote\n",
    "        \n",
    "        \n",
    "        estimated_error_rates = get_worker_error_rate_estimation(items_votes)\n",
    "        \n",
    "        for item_id in items_to_vote:\n",
    "            last_vote = list(items_votes[item_id].values())[-1]\n",
    "            last_worker_id = list(items_votes[item_id])[-1]\n",
    "            beliefs[item_id] = updateBelief(beliefs[item_id], last_vote, states_difficulties,\n",
    "                                     get_worker_error_rate(last_worker_id, estimated_error_rates, avg_error_rate, estimate_after, have_submitted))\n",
    "            \n",
    "        #print(f\"Num to vote: {len(items_to_vote)}\")\n",
    "    #end while\n",
    "            \n",
    "    return answers, items_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','wrong_cost','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 1000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"NormalConfusion\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = []\n",
    "acc_pos = np.random.normal(1, .2, workers_num)\n",
    "workers_error_rates.append([w_acc_pos * .5 for w_acc_pos in acc_pos]) # 10% higher than positive\n",
    "workers_error_rates.append(acc_pos) # possitive error rate 75\n",
    "\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for wrong_cost in wrong_answer_costs:\n",
    "        total_results = []\n",
    "\n",
    "        policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "        policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            answers, items_votes = solve_confusion(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "            costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "        # end for iterations\n",
    "\n",
    "        result = [f\"confusion-matrix-c{wrong_cost}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, wrong_cost, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate]\n",
    "\n",
    "        total_results.append(result)\n",
    "        pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results.csv\", mode='a', index=False, header=False)\n",
    "    #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion with different FNC and TPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['name','num_workers','workers_distribution','policy_name','num_items','data_bal','items_diff','num_states','cost','cost_std', 'recall','recall_std', 'precision', 'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'estimate_after', 'avg_error_rate', 'wce','wce_std','fnc','fpc']\n",
    "\n",
    "items_num = 1000\n",
    "possitive_percentage = 0.5\n",
    "item_difficulty = 0.5\n",
    "items_difficulties = [item_difficulty] * items_num\n",
    "items_ground_truth = generate_gold_data(items_num, possitive_percentage)\n",
    "loss_ratio = 5\n",
    "\n",
    "\n",
    "workers_num = 1000\n",
    "\n",
    "avg_error_rate = 1\n",
    "\n",
    "dist_name = \"NormalConfusion\"\n",
    "normal_mean = 1\n",
    "normal_std = 0.2\n",
    "\n",
    "workers_error_rates = []\n",
    "acc_pos = np.random.normal(1, .2, workers_num)\n",
    "workers_error_rates.append([w_acc_pos * .5 for w_acc_pos in acc_pos]) # 10% higher than positive\n",
    "workers_error_rates.append(acc_pos) # possitive error rate 75\n",
    "\n",
    "\n",
    "fncs = [-1,-5,-10,-500]\n",
    "fpcs = [-1,-5,-10,-500]\n",
    "\n",
    "states_num = 23\n",
    "policy_path = \"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/ModelLearning/Policies/\"\n",
    "\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "state_diff = getDifficulties(0.1)\n",
    "\n",
    "\n",
    "for moment_error_estimation in [False, True]:\n",
    "    for fnc in fncs:\n",
    "        for fpc in fpcs:\n",
    "            total_results = []\n",
    "\n",
    "            #policy_name = f\"23states_base-c{wrong_cost}.policy\"\n",
    "            policy_name = f'wrong-cost-fnc{fnc}-fpc{fpc}.policy'\n",
    "            policy = readPolicy(policy_path + policy_name, states_num)\n",
    "\n",
    "            losses = []\n",
    "            recalls = []\n",
    "            precisions = []\n",
    "            costs = []\n",
    "            f_ones = []\n",
    "            f_betas = []\n",
    "            wces = []\n",
    "\n",
    "            for _ in range(10):\n",
    "                answers, items_votes = solve_confusion(states_num, state_diff, avg_error_rate, policy, workers_error_rates, items_difficulties, items_ground_truth, moment_error_estimation)\n",
    "\n",
    "                costs.append(np.mean([len(v) for k,v in items_votes.items()]))    \n",
    "\n",
    "                loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics(answers, items_ground_truth, -1*fnc, -1*fpc)\n",
    "                losses.append(loss)\n",
    "                recalls.append(recall)\n",
    "                precisions.append(precision)\n",
    "                f_ones.append(f1)\n",
    "                f_betas.append(f_beta)\n",
    "                wces.append(wce)\n",
    "\n",
    "            result = [f\"confusion-matrix-fnc{fnc}-fpc{fpc}\", workers_num, dist_name+f\"({normal_mean},{normal_std})\", policy_name, items_num, possitive_percentage, \n",
    "                  item_difficulty, states_num, round_to_3(np.mean(costs)), round_to_3(np.std(costs)), round_to_3(np.mean(recalls)), round_to_3(np.std(recalls)),\n",
    "                 round_to_3(np.mean(precisions)), round_to_3(np.std(precisions)), round_to_3(np.mean(losses)), round_to_3(np.std(losses)),\n",
    "                 round_to_3(np.mean(f_ones)), round_to_3(np.std(f_ones)), round_to_3(np.mean(f_betas)), round_to_3(np.std(f_betas)), moment_error_estimation, avg_error_rate, \n",
    "                  round_to_3(np.mean(wces)), round_to_3(np.std(wces)), fnc, fpc]\n",
    "\n",
    "\n",
    "            total_results.append(result)\n",
    "            pd.DataFrame(total_results, columns=columns).to_csv(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/dai_pomdp/results/pomdp_results_diff.csv\", mode='a', index=False, header=False)\n",
    "        #end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check workers accs mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8916297807595583\n"
     ]
    }
   ],
   "source": [
    "def get_acc(diffs, e):\n",
    "    accs = []\n",
    "    for d in diffs:\n",
    "        accs.append(1/2 * (1 + math.pow((1 - d),e)))\n",
    "\n",
    "    return np.mean(accs)\n",
    "\n",
    "diffs = np.arange(0, 1.1, 0.1)\n",
    "workers_error = np.random.normal(.2,.01,1000)\n",
    "workers_acc = [get_acc(diffs, e) for e in workers_error]\n",
    "\n",
    "print(np.mean(workers_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and evaluate Dai'sresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.genPOMDP import *\n",
    "from os import system\n",
    "\n",
    "diffs = [[0,1], #5 states\n",
    "         [0, 0.33, 0.66, 1], #9 states\n",
    "         [0, 0.2, 0.4, 0.6, 0.8, 1], # 13 states\n",
    "         [0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1] #17 states\n",
    "        ]\n",
    "\n",
    "wrong_answer_costs = [10,20,50,500,1000]\n",
    "\n",
    "#filename, wronganswercost,createjobcost,distributionavg,numpools\n",
    "for wrong_cost in wrong_answer_costs:\n",
    "    for diff in diffs:\n",
    "        num_st = len(diff) * 2 + 1\n",
    "        name = f'{num_st}states-c{wrong_cost}'\n",
    "        genPOMDP(f'log/pomdp/{name}.pomdp', -1 * wrong_cost, [1], [1], 1, diff)\n",
    "        system(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/zmdp-1.1.7/bin/darwin18/zmdp solve log/pomdp/{name}.pomdp -o ModelLearning/Policies/{name}.policy -t 300\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 23 states base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.genPOMDP import *\n",
    "from os import system\n",
    "from ModelLearning.utils import *\n",
    "diff = getDifficulties(0.1)\n",
    "num_st = len(diff)\n",
    "wrong_answer_costs = [10,20,50,500,1000,5000]\n",
    "\n",
    "#worker error rate Normal(1,0.2)\n",
    "\n",
    "for wrong_cost in wrong_answer_costs:\n",
    "    genPOMDP(f'log/pomdp/23states_base-c{wrong_cost}.pomdp', -1 * wrong_cost, [1], [1], 1, diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wrong_cost in wrong_answer_costs:\n",
    "    system(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/zmdp-1.1.7/bin/darwin18/zmdp solve log/pomdp/23states_base-c{wrong_cost}.pomdp -o ModelLearning/Policies/23states_base-c{wrong_cost}.policy -t 300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 24states with unclassified pomdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.genPOMDP import *\n",
    "from os import system\n",
    "from ModelLearning.utils import *\n",
    "diff = getDifficulties(0.1)\n",
    "num_st = len(diff)\n",
    "wrong_cost = -500\n",
    "genPOMDP('log/pomdp/unclassified.pomdp', wrong_cost, [1], [1], 1, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0)(1,0)(0,1)(1,1)\n",
      "\n",
      "(0,0)(0.33,0)(0.66,0)(1,0)(0,1)(0.33,1)(0.66,1)(1,1)\n",
      "\n",
      "(0,0)(0.2,0)(0.4,0)(0.6,0)(0.8,0)(1,0)(0,1)(0.2,1)(0.4,1)(0.6,1)(0.8,1)(1,1)\n",
      "\n",
      "(0,0)(0.15,0)(0.3,0)(0.45,0)(0.6,0)(0.75,0)(0.9,0)(1,0)(0,1)(0.15,1)(0.3,1)(0.45,1)(0.6,1)(0.75,1)(0.9,1)(1,1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for diff_v in diffs:\n",
    "    for c in [0,1]:\n",
    "        for diff in diff_v:\n",
    "            print(f\"({diff},{c})\", end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "\n",
    "sts = [5, 9, 13, 17]\n",
    "for st in sts:\n",
    "    system(f\"/bin/bash run_experiment.sh 300 1000 0.5 {st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(f\"/bin/bash run_experiment.sh 300 1000 0.5 24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results and show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "Num States: 23\n",
      "Results for cost: 500:\n",
      "Cost mean: 7.764 - Std: 0.309\n",
      "Recall mean: 0.964 - Std: 0.012\n",
      "Precision mean: 0.966 - Std: 0.009\n",
      "Loss mean: 0.106 - Std: 0.033\n",
      "F-1: 0.965 - Std: 0.01\n",
      "F-beta: 0.965 - Std: 0.011\n",
      "  ---  \n",
      "unclassified\n",
      "Num States: 23\n",
      "Results for cost: 500:\n",
      "Cost mean: 7.88 - Std: 0.481\n",
      "Recall mean: 0.955 - Std: 0.023\n",
      "Precision mean: 0.956 - Std: 0.018\n",
      "Loss mean: 0.135 - Std: 0.067\n",
      "F-1: 0.956 - Std: 0.02\n",
      "F-beta: 0.955 - Std: 0.022\n",
      "  ---  \n",
      "5states\n",
      "Num States: 5\n",
      "Results for cost: 500:\n",
      "Cost mean: 1.0 - Std: 0.0\n",
      "Recall mean: 0.766 - Std: 0.013\n",
      "Precision mean: 0.761 - Std: 0.015\n",
      "Loss mean: 0.716 - Std: 0.039\n",
      "F-1: 0.764 - Std: 0.011\n",
      "F-beta: 0.765 - Std: 0.011\n",
      "  ---  \n",
      "9states\n",
      "Num States: 9\n",
      "Results for cost: 500:\n",
      "Cost mean: 5.9 - Std: 0.063\n",
      "Recall mean: 0.947 - Std: 0.007\n",
      "Precision mean: 0.948 - Std: 0.007\n",
      "Loss mean: 0.163 - Std: 0.022\n",
      "F-1: 0.947 - Std: 0.007\n",
      "F-beta: 0.947 - Std: 0.007\n",
      "  ---  \n",
      "13states\n",
      "Num States: 13\n",
      "Results for cost: 500:\n",
      "Cost mean: 7.276 - Std: 0.104\n",
      "Recall mean: 0.955 - Std: 0.011\n",
      "Precision mean: 0.965 - Std: 0.007\n",
      "Loss mean: 0.127 - Std: 0.029\n",
      "F-1: 0.96 - Std: 0.008\n",
      "F-beta: 0.957 - Std: 0.01\n",
      "  ---  \n",
      "17states\n",
      "Num States: 17\n",
      "Results for cost: 500:\n",
      "Cost mean: 7.254 - Std: 0.123\n",
      "Recall mean: 0.96 - Std: 0.009\n",
      "Precision mean: 0.954 - Std: 0.012\n",
      "Loss mean: 0.122 - Std: 0.028\n",
      "F-1: 0.957 - Std: 0.009\n",
      "F-beta: 0.959 - Std: 0.009\n",
      "  ---  \n",
      "bimodal\n",
      "Num States: 23\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.127 - Std: 0.202\n",
      "Recall mean: 0.876 - Std: 0.027\n",
      "Precision mean: 0.875 - Std: 0.018\n",
      "Loss mean: 0.381 - Std: 0.067\n",
      "F-1: 0.875 - Std: 0.019\n",
      "F-beta: 0.876 - Std: 0.024\n",
      "  ---  \n"
     ]
    }
   ],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def round_to_3(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev = 1000,4.000000,0.300000,1000,4.000000,0.300000\n",
    "#name = \"%d,%.2f,%.2f,%d,%.2f,%.2f\" % (WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev)\n",
    "\n",
    "\n",
    "results = { 'base': {'path': 'base - 100,1.00,0.20,100,1.00,0.20', 'states': '23'},\n",
    "            'unclassified': {'path': 'unclassified - 100,1.00,0.20,100,1.00,0.20', 'states': '23'},\n",
    "            '5states': {'path': 's5 - 100,1.00,0.20,100,1.00,0.20', 'states': '5'},\n",
    "            '9states': {'path': 's9 - 100,1.00,0.20,100,1.00,0.20', 'states': '9'},\n",
    "            '13states': {'path': 's13 - 100,1.00,0.20,100,1.00,0.20', 'states': '13'},\n",
    "            '17states': {'path': 's17 - 100,1.00,0.20,100,1.00,0.20', 'states': '17'},\n",
    "            'bimodal': {'path': 'diff_rates - 100,1.00,0.20,100,1.00,0.20', 'states': '23'},\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(name)\n",
    "    st = result['states']\n",
    "    print(f\"Num States: {st}\")\n",
    "    \n",
    "    RELATIVEPATH = f\"Experiments/{result['path']}\"\n",
    "\n",
    "    NUMBER_OF_REPETITIONS = 5\n",
    "    loss_ratio = 5\n",
    "    total_cost_mean = 0.\n",
    "    initial_line = 8\n",
    "    #wrong_answer_costs = [10,500,1000,5000,10000,15000]\n",
    "    wrong_answer_costs = [500]\n",
    "    NUMBER_OF_ITEMS = 1000\n",
    "    worker_num = 100\n",
    "    data_bal = 0.5\n",
    "\n",
    "    for wrongAnswerCost in wrong_answer_costs:\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "\n",
    "        for iteration in range(NUMBER_OF_REPETITIONS):\n",
    "            iteration_results_file = open(RELATIVEPATH + \"/%d/Unstarred/%d\" % (wrongAnswerCost,iteration),'r')\n",
    "            gt_vals = []\n",
    "            classification_vals = []\n",
    "            iteration_cost = []\n",
    "            for i, line in enumerate(iteration_results_file):\n",
    "                if (initial_line - 2 < i < (initial_line + NUMBER_OF_ITEMS - 1)):\n",
    "                    item_id, y_val, gt_val, item_cost, item_difficulty, item_real_difficulty,  = line.rstrip().split(\",\")\n",
    "                    classification_vals.append(int(y_val))\n",
    "                    gt_vals.append(int(gt_val))\n",
    "                    iteration_cost.append(float(item_cost))\n",
    "            # end for results file\n",
    "\n",
    "            costs.append(np.mean(iteration_cost))    \n",
    "            loss, recall, precision, f1, beta, f_beta = alg_utils.Metrics.compute_metrics(classification_vals, gt_vals, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "        # end for iterations\n",
    "\n",
    "        print(f\"Results for cost: {wrongAnswerCost}:\")\n",
    "        print(f\"Cost mean: {round_to_3(np.mean(costs))} - Std: {round_to_3(np.std(costs))}\")\n",
    "        print(f\"Recall mean: {round_to_3(np.mean(recalls))} - Std: {round_to_3(np.std(recalls))}\")\n",
    "        print(f\"Precision mean: {round_to_3(np.mean(precisions))} - Std: {round_to_3(np.std(precisions))}\")\n",
    "        print(f\"Loss mean: {round_to_3(np.mean(losses))} - Std: {round_to_3(np.std(losses))}\")\n",
    "        print(f\"F-1: {round_to_3(np.mean(f_ones))} - Std: {round_to_3(np.std(f_ones))}\")\n",
    "        print(f\"F-beta: {round_to_3(np.mean(f_betas))} - Std: {round_to_3(np.std(f_betas))}\")\n",
    "\n",
    "        norm_cost = round_to_3(np.mean(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        norm_cost_std = round_to_3(np.std(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "\n",
    "        print(\"  ---  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dai's Results over real-world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BarzanMozafari\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.726 - Std: 0.342\n",
      "Recall mean: 0.914 - Std: 0.015\n",
      "Precision mean: 0.907 - Std: 0.015\n",
      "Loss mean: 0.264 - Std: 0.041\n",
      "5,0.05,500,BarzanMozafari,83,4.0-0.42,0.491,536.3000000000001,17.1,0.264,0.041,0.914,0.015,0.907,0.015\n",
      "  ---  \n",
      "RTE\n",
      "Results for cost: 500:\n",
      "Cost mean: 3.743 - Std: 0.042\n",
      "Recall mean: 0.98 - Std: 0.006\n",
      "Precision mean: 0.981 - Std: 0.006\n",
      "Loss mean: 0.061 - Std: 0.012\n",
      "5,0.05,500,RTE,164,4.0-0.15,0.5,149.72,1.6800000000000002,0.061,0.012,0.98,0.006,0.981,0.006\n",
      "  ---  \n",
      "SpamCF\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.355 - Std: 0.274\n",
      "Recall mean: 0.92 - Std: 0.025\n",
      "Precision mean: 0.974 - Std: 0.019\n",
      "Loss mean: 0.3 - Std: 0.079\n",
      "5,0.05,500,SpamCF,150,4.0-0.4,0.3069306930693069,52.292750000000005,1.3837000000000002,0.3,0.079,0.92,0.025,0.974,0.019\n",
      "  ---  \n",
      "TEMP\n",
      "Results for cost: 500:\n",
      "Cost mean: 3.602 - Std: 0.051\n",
      "Recall mean: 0.978 - Std: 0.005\n",
      "Precision mean: 0.985 - Std: 0.002\n",
      "Loss mean: 0.07 - Std: 0.019\n",
      "5,0.05,500,TEMP,76,4.0-0.14,0.4393939393939394,83.20620000000001,1.1781,0.07,0.019,0.978,0.005,0.985,0.002\n",
      "  ---  \n",
      "WVSCM\n",
      "Results for cost: 500:\n",
      "Cost mean: 13.288 - Std: 0.332\n",
      "Recall mean: 0.792 - Std: 0.032\n",
      "Precision mean: 0.873 - Std: 0.029\n",
      "Loss mean: 0.703 - Std: 0.122\n",
      "5,0.05,500,WVSCM,17,4.0-0.5,0.3625,106.304,2.6560000000000006,0.703,0.122,0.792,0.032,0.873,0.029\n",
      "  ---  \n"
     ]
    }
   ],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def round_to_3(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev = 1000,4.000000,0.300000,1000,4.000000,0.300000\n",
    "#name = \"%d,%.2f,%.2f,%d,%.2f,%.2f\" % (WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev)\n",
    "\n",
    "datasets = ['BarzanMozafari', 'RTE', 'SpamCF', 'TEMP', 'WVSCM']\n",
    "RELATIVEPATH = \"Experiments/Results/RealWorld/\"\n",
    "\n",
    "NUMBER_OF_REPETITIONS = 6\n",
    "loss_ratio = 5\n",
    "total_cost_mean = 0.\n",
    "initial_line = 8\n",
    "wrong_answer_costs = [500]\n",
    "\n",
    "gammas_dist = [(4.,0.42),(4.,0.15),(4.,0.4),(4.,0.14),(4.,0.5)]\n",
    "\n",
    "for key,name in enumerate(datasets):\n",
    "    print(name)\n",
    "\n",
    "    for wrongAnswerCost in wrong_answer_costs:\n",
    "        ground_truth, workers_accuracy = alg_utils.get_real_dataset_data(name)\n",
    "        NUMBER_OF_ITEMS = len(ground_truth)\n",
    "        \n",
    "        data_bal = sum(ground_truth) / NUMBER_OF_ITEMS\n",
    "        worker_num = len(workers_accuracy)\n",
    "        \n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "\n",
    "        for iteration in range(NUMBER_OF_REPETITIONS):\n",
    "            iteration_results_file = open(RELATIVEPATH + \"%s/%d/Unstarred/%d\" % (name,wrongAnswerCost,iteration),'r')\n",
    "            gt_vals = []\n",
    "            classification_vals = []\n",
    "            iteration_cost = 0.\n",
    "            for i, line in enumerate(iteration_results_file):\n",
    "                if (initial_line - 1 < i < (initial_line + NUMBER_OF_ITEMS - 1)):\n",
    "                    item_id, y_val, gt_val, item_cost, item_difficulty, item_real_difficulty,  = line.rstrip().split(\",\")\n",
    "                    classification_vals.append(int(y_val))\n",
    "                    gt_vals.append(int(gt_val))\n",
    "                    iteration_cost += float(item_cost)\n",
    "            # end for results file\n",
    "\n",
    "            costs.append(iteration_cost / NUMBER_OF_ITEMS)    \n",
    "            loss, recall, precision = alg_utils.Metrics.compute_metrics(classification_vals, gt_vals, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "        # end for iterations\n",
    "\n",
    "        print(f\"Results for cost: {wrongAnswerCost}:\")\n",
    "        print(f\"Cost mean: {round_to_3(np.mean(costs))} - Std: {round_to_3(np.std(costs))}\")\n",
    "        print(f\"Recall mean: {round_to_3(np.mean(recalls))} - Std: {round_to_3(np.std(recalls))}\")\n",
    "        print(f\"Precision mean: {round_to_3(np.mean(precisions))} - Std: {round_to_3(np.std(precisions))}\")\n",
    "        print(f\"Loss mean: {round_to_3(np.mean(losses))} - Std: {round_to_3(np.std(losses))}\")\n",
    "        \n",
    "        norm_cost = round_to_3(np.mean(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        norm_cost_std = round_to_3(np.std(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        \n",
    "        shape,scale = gammas_dist[key]\n",
    "        norm_gamma = f\"{shape}-{scale}\"\n",
    "        \n",
    "        #loss_ratio,cost_ratio,wrong_answer_cost,name,workers_num,gamma_dist,data_bal,cost,cost_std,loss,loss_std,recall,recall_std,precision,precision_std\n",
    "        print(f\"5,0.05,500,{name},{worker_num},{norm_gamma},{data_bal},{norm_cost},{norm_cost_std},{round_to_3(np.mean(losses))},{round_to_3(np.std(losses))},{round_to_3(np.mean(recalls))},{round_to_3(np.std(recalls))},{round_to_3(np.mean(precisions))},{round_to_3(np.std(precisions))}\")\n",
    "        \n",
    "        print(\"  ---  \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test compute better values for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "def myfindBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def myfindBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def findValues(actions, policy, beliefState):\n",
    "    bestValue = [-1230981239102938019] * len(actions)\n",
    "\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue[action]:\n",
    "                bestValue[action] = value\n",
    "\n",
    "    return bestValue\n",
    "\n",
    "numberOfWorkerPools = 1\n",
    "numStates = 23\n",
    "numberOfProblems = 100\n",
    "actions = range(0, numberOfWorkerPools+3) # 4 w/ unclassified\n",
    "\n",
    "#belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief = [1] * 11 + [1] * 12 #[1 for i in range(numStates)]\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "#policies = {'frtdp':'/Users/pmaglione/Documents/pomdp_solve_test/frtdp.policy',\n",
    "#            'hsvi':'/Users/pmaglione/Documents/pomdp_solve_test/hsvi.policy'}\n",
    "\n",
    "policies = {'unclassified': '/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/Policies/unclassified250.policy'}\n",
    "\n",
    "\n",
    "#values = {'frtdp':0, 'hsvi':0}\n",
    "\n",
    "for key,path in policies.items():\n",
    "    policy = readPolicy(path, numStates)\n",
    "    beliefState = beliefs[0]\n",
    "    #print(actions)\n",
    "    print(f\"Strategy: {key}\")\n",
    "    key_vals = findValues(actions, policy, beliefState)\n",
    "    values[key] = key_vals\n",
    "    print(key_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numStates = 22\n",
    "numberOfProblems = 10\n",
    "belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "\n",
    "gammas = [0.628980,\n",
    "0.580959,\n",
    "1.281037,\n",
    "0.987592,\n",
    "1.373258,\n",
    "0.801062,\n",
    "1.488326,\n",
    "1.523445,\n",
    "1.448263,\n",
    "1.126245,\n",
    "1.046380,\n",
    "1.418695,\n",
    "1.118415,\n",
    "1.142464,\n",
    "1.032052,\n",
    "0.860525,\n",
    "1.418732]\n",
    "\n",
    "gammas = [1.281037] * 100\n",
    "\n",
    "difficulties = getDifficulties(0.1)\n",
    "gen_belief = beliefs[0]\n",
    "observation = 1\n",
    "\n",
    "num = 0\n",
    "\n",
    "for gamma in gammas:\n",
    "    gen_belief = updateBelief(gen_belief, observation, difficulties, gamma)\n",
    "    action = findBestAction([0,1,2], policy, gen_belief)\n",
    "    print(action)\n",
    "    print(gen_belief)\n",
    "    #print(gen_belief.index(max(gen_belief)))\n",
    "    print(\"---\")\n",
    "    num = num + 1\n",
    "    if (action != 0):\n",
    "        print(num)\n",
    "        break\n",
    "    observation = np.random.binomial(1, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "def myfindBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def myfindBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def findValues(actions, policy, beliefState):\n",
    "    bestValue = [-1230981239102938019] * len(actions)\n",
    "\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue[action]:\n",
    "                bestValue[action] = value\n",
    "\n",
    "    return bestValue\n",
    "\n",
    "numberOfWorkerPools = 1\n",
    "numStates = 23\n",
    "numberOfProblems = 100\n",
    "actions = range(0, numberOfWorkerPools+3)\n",
    "\n",
    "belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "def myReadPolicy(path, numStates):\n",
    "    f = open(path, \"r\")\n",
    "    alpha_vectors = {0: [], 1:[], 2:[]}\n",
    "\n",
    "    action = -1\n",
    "    for line in f:\n",
    "        if len(line) == 2:\n",
    "            action = int(line)\n",
    "        elif len(line) > 2:\n",
    "            alpha_vectors[action].append([float(x) for x in line.split()])\n",
    "    \n",
    "    return alpha_vectors\n",
    "    \n",
    "\n",
    "path = '/Users/pmaglione/Documents/pomdp_solve_test/pomdp-solve'\n",
    "values = {}\n",
    "\n",
    "methods = ['enum','twopass','witness','incprune']\n",
    "beliefState = beliefs[0]\n",
    "#print(f\"BeliefState: {beliefState}\")\n",
    "    \n",
    "#for method in methods:\n",
    "file = f\"/Users/pmaglione/Documents/pomdp_solve_test/pomdp-solve/unclassified.policy.alpha\"\n",
    "\n",
    "policy = myReadPolicy(file, numStates)\n",
    "\n",
    "#print(f\"Strategy: {method}\")\n",
    "key_vals = findValues(actions, policy, beliefState)\n",
    "values[key] = key_vals\n",
    "print(key_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 states POMDP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from truth_finder import expectation_maximization\n",
    "\n",
    "#metrics\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    #k penalization for false negatives\n",
    "    def compute_metrics(items_classification, gt, lr = 1):\n",
    "        # FP == False Inclusion\n",
    "        # FN == False Exclusion\n",
    "        fp = fn = tp = tn = 0.\n",
    "        for i in range(len(gt)):\n",
    "            gt_val = gt[i]\n",
    "            cl_val = items_classification[i]\n",
    "\n",
    "            if gt_val and not cl_val:\n",
    "                fn += 1\n",
    "            if not gt_val and cl_val:\n",
    "                fp += 1\n",
    "            if gt_val and cl_val:\n",
    "                tp += 1\n",
    "            if not gt_val and not cl_val:\n",
    "                tn += 1\n",
    "                        \n",
    "\n",
    "        recall = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        loss = (fp + (fn * lr)) / len(gt)\n",
    "        \n",
    "        return loss, recall, precision\n",
    "#end\n",
    "\n",
    "def simulate_workers(workers_num, cheaters_prop, fixed_acc, workers_acc, base_acc = .5):\n",
    "    workers = {}\n",
    "    for i in range(workers_num):\n",
    "        if (fixed_acc == False):\n",
    "            if np.random.binomial(1, cheaters_prop):\n",
    "                # worker_type is 'rand_ch'\n",
    "                worker_acc_pos = worker_acc_neg = 0.5\n",
    "            else:\n",
    "                # worker_type is 'worker'\n",
    "                worker_acc_pos = base_acc + (np.random.beta(1, 1) * (1 - base_acc))\n",
    "                #worker_acc_neg = worker_acc_pos + 0.1 if worker_acc_pos + 0.1 <= 1. else 1.\n",
    "                worker_acc_neg = worker_acc_pos\n",
    "        else:\n",
    "            worker_acc_pos = workers_acc\n",
    "            worker_acc_neg = worker_acc_pos\n",
    "\n",
    "        workers[i] = [worker_acc_pos, worker_acc_neg]\n",
    "\n",
    "    return workers\n",
    "\n",
    "def get_random_worker_accuracy(workers_accuracy, item_id, votes):\n",
    "    item_votes = votes[item_id].copy()\n",
    "    worker_ids_used = item_votes.keys()\n",
    "    workers_ids_range = workers_accuracy.keys()\n",
    "    workers_ids_unused = [val for val in workers_ids_range if val not in worker_ids_used]\n",
    "    \n",
    "    if (len(workers_ids_unused) == 0):\n",
    "        used = len(worker_ids_used)\n",
    "        ranges = len(workers_ids_range)\n",
    "        unu = len(workers_ids_unused)\n",
    "        print(f'used: {used}')\n",
    "        print(f'workers: {ranges}')\n",
    "        print(f'unused: {unu}')\n",
    "        raise ValueError(\"Unused empty!?\")\n",
    "    \n",
    "    selected_worker_id = np.random.choice(workers_ids_unused)\n",
    "    worker_acc_pos = workers_accuracy[selected_worker_id][0]\n",
    "    worker_acc_neg = workers_accuracy[selected_worker_id][1]\n",
    "\n",
    "    return {'worker_id': selected_worker_id, 'acc_pos':worker_acc_pos, 'acc_neg': worker_acc_neg}\n",
    "\n",
    "\n",
    "def get_worker_vote(workers_accuracy, i, gt, votes):\n",
    "    worker_data = get_random_worker_accuracy(workers_accuracy, i, votes)\n",
    "    worker_id, worker_acc_pos, worker_acc_neg = worker_data['worker_id'], worker_data['acc_pos'], worker_data['acc_neg']\n",
    "\n",
    "    if (gt[i]):\n",
    "        worker_acc = worker_acc_pos\n",
    "    else:\n",
    "        worker_acc = worker_acc_neg\n",
    "\n",
    "    if np.random.binomial(1, worker_acc):\n",
    "        vote = gt[i]\n",
    "    else:\n",
    "        vote = 1 - gt[i]\n",
    "\n",
    "    return (worker_id, vote)\n",
    "\n",
    "def generate_gold_data(items_num, possitive_percentage):\n",
    "    pos_items_number = int(round(((possitive_percentage * 100) * items_num) / 100))     \n",
    "    gold_data = ([1] * pos_items_number) + ([0] * (items_num - pos_items_number))\n",
    "    random.shuffle(gold_data)\n",
    "\n",
    "    return gold_data\n",
    "\n",
    "def findBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def findBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = findBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def myReadPolicy(path, states, origin):\n",
    "    if origin != 'zmdp':\n",
    "        f = open(path, \"r\")\n",
    "        alpha_vectors = {0: [], 1:[], 2:[]}\n",
    "\n",
    "        action = -1\n",
    "        for line in f:\n",
    "            if len(line) == 2:\n",
    "                action = int(line)\n",
    "            elif len(line) > 2:\n",
    "                strategy_vectors = [float(x) for x in line.split()]\n",
    "                if len(strategy_vectors) == states:\n",
    "                    alpha_vectors[action].append(strategy_vectors)\n",
    "    else:\n",
    "        alpha_vectors = readPolicy(path, states)\n",
    "    \n",
    "    return alpha_vectors\n",
    "\n",
    "def update_belief(prevBelief, vote, acc, states_per_class):\n",
    "    newBeliefs = []\n",
    "    for i in range(0, 2):  # 0,1\n",
    "        for j in range(0, states_per_class):\n",
    "            state = (i * states_per_class) + j\n",
    "            \n",
    "            if vote == i:\n",
    "                newBeliefs.append(acc * prevBelief[state])\n",
    "            else:\n",
    "                newBeliefs.append((1 - acc) * prevBelief[state])\n",
    "                \n",
    "    newBeliefs.append(0.0)\n",
    "    normalize(newBeliefs)\n",
    "    return newBeliefs\n",
    "\n",
    "def input_adapter(responses):\n",
    "    '''\n",
    "    :param responses:\n",
    "    :return: Psi\n",
    "    '''\n",
    "    Psi = [[] for _ in responses.keys()]\n",
    "    i = 0\n",
    "    for obj_id, obj_responses in responses.items():\n",
    "        k = 0\n",
    "        for worker_id, worker_response in obj_responses.items():\n",
    "            Psi[i].append((k, worker_response[0]))\n",
    "            k += 1\n",
    "        i += 1\n",
    "    return Psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(policy, items, states, num_actions, workers_accuracy, ground_truth, acc):\n",
    "    belief = [1 for i in range(states)]  # init , equivalent to [1] * numStates\n",
    "    belief[states-1] = 0  # last states = 0, terminating state\n",
    "    belief = normalize(belief)\n",
    "    beliefs = [deepcopy(belief) for i in range(items)]\n",
    "    beliefState = beliefs[0]\n",
    "    actions = range(0, num_actions)\n",
    "    items_classification = {}\n",
    "    votes = {i:{} for i in range(items)}\n",
    "\n",
    "    states_per_class = int((states - 1) / 2)\n",
    "\n",
    "    while len(items_classification) != items:\n",
    "        for item_id, belief_state in enumerate(beliefs):\n",
    "            if item_id not in items_classification.keys(): # if item is classified\n",
    "                best_action = findBestAction(actions, policy, belief_state)\n",
    "\n",
    "                #prevent\n",
    "                if len(votes[item_id]) > 100:\n",
    "                    mv = (sum(votes[item_id]) / len(votes[item_id])) > 0.5\n",
    "                    if mv:\n",
    "                        best_action = 2\n",
    "                    else:\n",
    "                        best_action = 1\n",
    "\n",
    "                if best_action == 0:\n",
    "                    worker_id, vote = get_worker_vote(workers_accuracy, item_id, ground_truth, votes)\n",
    "                    votes[item_id][worker_id] = [vote]\n",
    "                    \n",
    "                    belief_state = update_belief(belief_state, vote, acc, states_per_class)\n",
    "                    \n",
    "                    beliefs[item_id] = belief_state\n",
    "                elif best_action == 1: #submit zero\n",
    "                    items_classification[item_id] = 0\n",
    "                else: #submit one\n",
    "                    items_classification[item_id] = 1\n",
    "                    \n",
    "        #end for\n",
    "        if min([len(v) for a,v in votes.items()]) == 3:\n",
    "            accs, p = expectation_maximization(max([len(v) for a,v in votes.items()]), items, input_adapter(votes))\n",
    "            acc = np.mean(accs)\n",
    "        \n",
    "    #end while\n",
    "    num_votes = sum([len(v) for k,v in votes.items()])\n",
    "\n",
    "    return items_classification, num_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true_percentage = .5\n",
    "states = 3\n",
    "items = 100\n",
    "num_actions = 3\n",
    "lr = 5\n",
    "acc = .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc):\n",
    "    ground_truth = generate_gold_data(items, data_true_percentage)\n",
    "    workers_accuracy = simulate_workers(1000, 0, False, acc, base_acc = .5)\n",
    "    \n",
    "    losses  = []\n",
    "    recalls  = []\n",
    "    precisions  = []\n",
    "    votes_amount = []\n",
    "    for _ in range(50):\n",
    "        items_classification, num_votes = run_exp(policy, items, states, num_actions, workers_accuracy, ground_truth, acc)\n",
    "        \n",
    "        votes_amount.append(num_votes / items) #average\n",
    "        \n",
    "        loss,  recall, precision = Metrics.compute_metrics(items_classification, ground_truth, lr)\n",
    "        losses.append(loss)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    print(f\" Votes: {np.mean(votes_amount)} / {np.std(votes_amount)} \\n Loss: {np.mean(losses)} / {np.std(losses)} \\n Recall: {np.mean(recalls)} / {np.std(recalls)} \\n Precision: {np.mean(precisions)} / {np.std(precisions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Votes: 9.6322 / 0.9893680609358683 \n",
      " Loss: 0.0238 / 0.03205557673790942 \n",
      " Recall: 0.992 / 0.01200000000000001 \n",
      " Precision: 0.9925161202942715 / 0.012344492992408751\n",
      " --- \n",
      " Votes: 10.5534 / 1.190007747873937 \n",
      " Loss: 0.0068000000000000005 / 0.014891608375189026 \n",
      " Recall: 0.9980000000000001 / 0.006000000000000005 \n",
      " Precision: 0.9964705882352942 / 0.007533087338156308\n",
      " --- \n",
      " Votes: 1.0 / 0.0 \n",
      " Loss: 0.7298 / 0.16922753913001276 \n",
      " Recall: 0.7568 / 0.06564876236457166 \n",
      " Precision: 0.7577381369027768 / 0.04809981928535244\n",
      " --- \n"
     ]
    }
   ],
   "source": [
    "policies = []\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-500/test.alpha', states, 'pomdp-solve'))\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-1000/test.alpha', states, 'pomdp-solve'))\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-10/test.alpha', states, 'pomdp-solve'))\n",
    "\n",
    "for policy in policies:\n",
    "    run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc)\n",
    "    print(\" --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Votes: 3.1920000000000006 / 0.17577258034175863 \n",
      " Loss: 0.2926 / 0.08666741025322032 \n",
      " Recall: 0.9036 / 0.033392214661504556 \n",
      " Precision: 0.8992675825856574 / 0.040620210608958304\n"
     ]
    }
   ],
   "source": [
    "states = 23\n",
    "policy = myReadPolicy('./ModelLearning/Policies/W1_COST500.policy', states, 'zmdp')\n",
    "run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare workers error rate estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#generate votes\n",
    "numWorkers = 10\n",
    "numberOfProblems = 10\n",
    "numVotes = 10\n",
    "votes = {}\n",
    "\n",
    "def get_worker_to_vote(numWorkers, votes):\n",
    "    found = False\n",
    "    while found is False:\n",
    "        worker_id = random.randint(1,numWorkers)\n",
    "        if worker_id not in votes.keys():\n",
    "            found = True\n",
    "            \n",
    "    return worker_id\n",
    "        \n",
    "for pn in range(numberOfProblems):\n",
    "    votes[pn] = {}\n",
    "    for i in range(numVotes):\n",
    "        worker_id = get_worker_to_vote(numWorkers, votes[pn])\n",
    "        observation = np.random.binomial(1, .75)\n",
    "        votes[pn][worker_id] = observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgentHunt_single.ModelLearning.utils import *\n",
    "from AgentHunt_single.Data import *\n",
    "from AgentHunt_single.Ballots import *\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "#from random import random\n",
    "from os import mkdir, rmdir\n",
    "from copy import deepcopy\n",
    "from math import floor\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 0, 6: 1, 1: 2, 5: 3, 10: 4, 2: 5, 3: 6, 9: 7, 7: 8, 8: 9}\n",
      "{4: 0.685824, 6: 0.830336, 1: 0.686345, 5: 0.562498, 10: 0.932432, 2: 0.027327, 3: 0.943438, 9: 0.231426, 7: 0.73242, 8: 0.511309}\n"
     ]
    }
   ],
   "source": [
    "EMPATH = '/Users/pmaglione/Repos/adaptive-pomdp-solutions/AgentHunt_single/EM/em'\n",
    "gammaList = [1]\n",
    "numberOfWorkerPools = 1\n",
    "ballots = Ballots(EMPATH)  # BALLOTS OBJECT\n",
    "\n",
    "fastLearning = False\n",
    "\n",
    "agentActions = [-1 for i in range(0, numberOfProblems)]\n",
    "\n",
    "observations = []\n",
    "for i in range(numberOfProblems):\n",
    "    observations.append([])\n",
    "    for j in range(numberOfWorkerPools):\n",
    "        observations.append([])\n",
    "\n",
    "for pn, item_votes in votes.items():\n",
    "    for worker_id, observation in item_votes.items():\n",
    "        observations[pn].append((observation, worker_id))\n",
    "\n",
    "bestAction = 1\n",
    "for i in range(numberOfProblems):\n",
    "    ballots.addQuestionAndRelearn(\n",
    "                            observations[i],  # all obs\n",
    "                            bestAction,  # 0/1\n",
    "                            .7,  # diff with higher %\n",
    "                            fastLearning)\n",
    "\n",
    "#print(votes)\n",
    "print(ballots.workersToIntegers)\n",
    "print(ballots.workersToGammas0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pomdp base with python-glad estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_glad.glad_bin as glad_bin\n",
    "\n",
    "def get_workers_order(votes):\n",
    "    worker_order = 0\n",
    "    worker_to_order = {}\n",
    "    order_to_worker = {}\n",
    "    num_votes = 0\n",
    "    num_workers = 0\n",
    "    num_items = len(votes)\n",
    "    \n",
    "    for item_id, item_votes in votes.items():\n",
    "        for worker_id, vote in item_votes.items():\n",
    "            num_votes += 1\n",
    "            if worker_id not in worker_to_order.keys():\n",
    "                worker_to_order[worker_id] = worker_order \n",
    "                order_to_worker[worker_order] = worker_id\n",
    "                worker_order += 1\n",
    "                \n",
    "    num_workers = len(worker_to_order)\n",
    "    \n",
    "    return num_items, num_workers, num_votes, worker_to_order, order_to_worker\n",
    "\n",
    "def writeToEMFormat(votes, path_em_input_file):\n",
    "    num_items, num_workers, num_votes, worker_to_order, order_to_worker = get_workers_order(votes)\n",
    "    \n",
    "    outputfile = open(path_em_input_file, 'w')\n",
    "    prior_beta = 0.5\n",
    "    #file headers\n",
    "    outputfile.write('%d %d %d %f\\n' % (num_votes, num_workers, num_items, prior_beta))\n",
    "    \n",
    "    #votes\n",
    "    for item_id, item_votes in votes.items():\n",
    "        for worker_id, vote in item_votes.items():\n",
    "            outputfile.write('%d %d %d\\n' % (item_id, worker_to_order[worker_id], vote))\n",
    "\n",
    "    outputfile.close()\n",
    "    \n",
    "    return worker_to_order, order_to_worker\n",
    "\n",
    "def get_worker_error_rate_estimation(votes, path_em_input_file):\n",
    "    # min 2 votes per item\n",
    "    if all(len(v) >= 2 for k, v in votes.items()):\n",
    "        worker_to_order, order_to_worker = writeToEMFormat(votes, path_em_input_file)\n",
    "\n",
    "        error_rates = glad_bin.estimate(path_em_input_file)\n",
    "\n",
    "        return {order_to_worker[worker_order]: abs(error_rate) for worker_order, error_rate in error_rates.items()}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "    \n",
    "    \n",
    "path_em_input_file = './log/em/ballots.eminput'\n",
    "\n",
    "#filename = '/Users/pmaglione/Repos/adaptive-pomdp-solutions/log/em/test_ballots_pure.eminput'\n",
    "#glad_bin.estimate(filename)\n",
    "error_rates = get_worker_error_rate_estimation(votes, path_em_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
