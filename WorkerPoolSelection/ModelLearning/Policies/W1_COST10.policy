# This file is a POMDP policy, represented as a set of "lower bound
# planes", each of which consists of an alpha vector and a corresponding
# action.  Given a particular belief b, this information can be used to
# answer two queries of interest:
#
#   1. What is a lower bound on the expected long-term reward starting
#        from belief b?
#   2. What is an action that achieves that expected reward lower bound?
#
# Each lower bound plane is only defined over a subset of the belief
# simplex--it is defined for those beliefs b such that the non-zero
# entries of b are a subset of the entries present in the plane's alpha
# vector.  If this condition holds we say the plane is 'applicable' to b.
#
# Given a belief b, both of the queries above can be answered by the
# following process: first, throw out all the planes that are not
# applicable to b.  Then, for each of the remaining planes, take the inner
# product of the plane's alpha vector with b.  The highest inner product
# value is the expected long-term reward lower bound, and the action label
# for that plane is the action that achieves the bound.

{
  policyType => "MaxPlanesLowerBound",
  numPlanes => 6,
  planes => [
    {
      action => 1,
      numEntries => 23,
      entries => [
        0, 1,
        1, 1,
        2, 1,
        3, 1,
        4, 1,
        5, 1,
        6, 1,
        7, 1,
        8, 1,
        9, 1,
        10, 1,
        11, -10,
        12, -10,
        13, -10,
        14, -10,
        15, -10,
        16, -10,
        17, -10,
        18, -10,
        19, -10,
        20, -10,
        21, -10,
        22, 0
      ]
    },
    {
      action => 2,
      numEntries => 23,
      entries => [
        0, -10,
        1, -10,
        2, -10,
        3, -10,
        4, -10,
        5, -10,
        6, -10,
        7, -10,
        8, -10,
        9, -10,
        10, -10,
        11, 1,
        12, 1,
        13, 1,
        14, 1,
        15, 1,
        16, 1,
        17, 1,
        18, 1,
        19, 1,
        20, 1,
        21, 1,
        22, 0
      ]
    },
    {
      action => 0,
      numEntries => 20,
      entries => [
        1, -0.653248,
        2, -1.29202,
        3, -1.91499,
        4, -2.52034,
        5, -3.10577,
        6, -3.66811,
        7, -4.20278,
        8, -4.70237,
        9, -5.15256,
        10, -5.49955,
        12, -0.653248,
        13, -1.29202,
        14, -1.91499,
        15, -2.52034,
        16, -3.10577,
        17, -3.66811,
        18, -4.20278,
        19, -4.70237,
        20, -5.15256,
        21, -5.49955
      ]
    },
    {
      action => 2,
      numEntries => 21,
      entries => [
        1, -10,
        2, -10,
        3, -10,
        4, -10,
        5, -10,
        6, -10,
        7, -10,
        8, -10,
        9, -10,
        10, -10,
        11, 1,
        12, 1,
        13, 1,
        14, 1,
        15, 1,
        16, 1,
        17, 1,
        18, 1,
        19, 1,
        20, 1,
        21, 1
      ]
    },
    {
      action => 1,
      numEntries => 21,
      entries => [
        0, 1,
        1, 1,
        2, 1,
        3, 1,
        4, 1,
        5, 1,
        6, 1,
        7, 1,
        8, 1,
        9, 1,
        10, 1,
        12, -10,
        13, -10,
        14, -10,
        15, -10,
        16, -10,
        17, -10,
        18, -10,
        19, -10,
        20, -10,
        21, -10
      ]
    },
    {
      action => 0,
      numEntries => 23,
      entries => [
        0, -0.0001,
        1, -0.653248,
        2, -1.29202,
        3, -1.91499,
        4, -2.52034,
        5, -3.10577,
        6, -3.66811,
        7, -4.20278,
        8, -4.70237,
        9, -5.15256,
        10, -5.49955,
        11, -0.0001,
        12, -0.653248,
        13, -1.29202,
        14, -1.91499,
        15, -2.52034,
        16, -3.10577,
        17, -3.66811,
        18, -4.20278,
        19, -4.70237,
        20, -5.15256,
        21, -5.49955,
        22, 0
      ]
    }
  ]
}
