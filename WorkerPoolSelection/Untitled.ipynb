{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and evaluate Dai'sresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.genPOMDP import *\n",
    "from os import system\n",
    "\n",
    "diffs = [[0,1], #5 states\n",
    "         [0, 0.33, 0.66, 1], #9 states\n",
    "         [0, 0.2, 0.4, 0.6, 0.8, 1], # 13 states\n",
    "         [0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1] #17 states\n",
    "        ]\n",
    "#wrong_answer_costs = [-10,-500,-1000,-5000,-10000,-15000]\n",
    "\n",
    "#filename, wronganswercost,createjobcost,distributionavg,numpools\n",
    "for wrong_cost in [-500]:\n",
    "    for diff in diffs:\n",
    "        num_st = len(diff)\n",
    "        genPOMDP('log/pomdp/rl.pomdp', wrong_cost, [1], [1], 1, diff)\n",
    "        system(f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/zmdp-1.1.7/bin/darwin18/zmdp solve log/pomdp/rl.pomdp -o ModelLearning/Policies/s{num_st}-{wrong_cost}.policy -t 300\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 24states with unclassified pomdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.genPOMDP import *\n",
    "from os import system\n",
    "from ModelLearning.utils import *\n",
    "diff = getDifficulties(0.1)\n",
    "num_st = len(diff)\n",
    "wrong_cost = -500\n",
    "genPOMDP('log/pomdp/unclassified.pomdp', wrong_cost, [1], [1], 1, diff)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0)(1,0)(0,1)(1,1)\n",
      "\n",
      "(0,0)(0.33,0)(0.66,0)(1,0)(0,1)(0.33,1)(0.66,1)(1,1)\n",
      "\n",
      "(0,0)(0.2,0)(0.4,0)(0.6,0)(0.8,0)(1,0)(0,1)(0.2,1)(0.4,1)(0.6,1)(0.8,1)(1,1)\n",
      "\n",
      "(0,0)(0.15,0)(0.3,0)(0.45,0)(0.6,0)(0.75,0)(0.9,0)(1,0)(0,1)(0.15,1)(0.3,1)(0.45,1)(0.6,1)(0.75,1)(0.9,1)(1,1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for diff_v in diffs:\n",
    "    for c in [0,1]:\n",
    "        for diff in diff_v:\n",
    "            print(f\"({diff},{c})\", end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "\n",
    "sts = [5, 9, 13, 17]\n",
    "for st in sts:\n",
    "    system(f\"/bin/bash run_experiment.sh 300 1000 0.5 {st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(f\"/bin/bash run_experiment.sh 300 1000 0.5 24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results and show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num States: 23\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.127 - Std: 0.202\n",
      "Recall mean: 0.876 - Std: 0.027\n",
      "Precision mean: 0.875 - Std: 0.018\n",
      "Loss mean: 0.381 - Std: 0.067\n",
      "  ---  \n"
     ]
    }
   ],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def round_to_3(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev = 1000,4.000000,0.300000,1000,4.000000,0.300000\n",
    "#name = \"%d,%.2f,%.2f,%d,%.2f,%.2f\" % (WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev)\n",
    "\n",
    "#sts = [5, 9, 13, 17]\n",
    "sts = [23]\n",
    "\n",
    "for st in sts:\n",
    "    print(f\"Num States: {st}\")\n",
    "    RELATIVEPATH = f\"Experiments/diff_rates - 100,1.00,0.20,100,1.00,0.20\"\n",
    "\n",
    "    NUMBER_OF_REPETITIONS = 5\n",
    "    loss_ratio = 5\n",
    "    total_cost_mean = 0.\n",
    "    initial_line = 8\n",
    "    #wrong_answer_costs = [10,500,1000,5000,10000,15000]\n",
    "    wrong_answer_costs = [500]\n",
    "    NUMBER_OF_ITEMS = 1000\n",
    "    worker_num = 100\n",
    "    data_bal = 0.5\n",
    "\n",
    "    for wrongAnswerCost in wrong_answer_costs:\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "\n",
    "        for iteration in range(NUMBER_OF_REPETITIONS):\n",
    "            iteration_results_file = open(RELATIVEPATH + \"/%d/Unstarred/%d\" % (wrongAnswerCost,iteration),'r')\n",
    "            gt_vals = []\n",
    "            classification_vals = []\n",
    "            iteration_cost = []\n",
    "            for i, line in enumerate(iteration_results_file):\n",
    "                if (initial_line - 2 < i < (initial_line + NUMBER_OF_ITEMS - 1)):\n",
    "                    item_id, y_val, gt_val, item_cost, item_difficulty, item_real_difficulty,  = line.rstrip().split(\",\")\n",
    "                    classification_vals.append(int(y_val))\n",
    "                    gt_vals.append(int(gt_val))\n",
    "                    iteration_cost.append(float(item_cost))\n",
    "            # end for results file\n",
    "\n",
    "            costs.append(np.mean(iteration_cost))    \n",
    "            loss, recall, precision = alg_utils.Metrics.compute_metrics(classification_vals, gt_vals, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "        # end for iterations\n",
    "\n",
    "        print(f\"Results for cost: {wrongAnswerCost}:\")\n",
    "        print(f\"Cost mean: {round_to_3(np.mean(costs))} - Std: {round_to_3(np.std(costs))}\")\n",
    "        print(f\"Recall mean: {round_to_3(np.mean(recalls))} - Std: {round_to_3(np.std(recalls))}\")\n",
    "        print(f\"Precision mean: {round_to_3(np.mean(precisions))} - Std: {round_to_3(np.std(precisions))}\")\n",
    "        print(f\"Loss mean: {round_to_3(np.mean(losses))} - Std: {round_to_3(np.std(losses))}\")\n",
    "\n",
    "        norm_cost = round_to_3(np.mean(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        norm_cost_std = round_to_3(np.std(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "\n",
    "        print(\"  ---  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dai's Results over real-world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BarzanMozafari\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.726 - Std: 0.342\n",
      "Recall mean: 0.914 - Std: 0.015\n",
      "Precision mean: 0.907 - Std: 0.015\n",
      "Loss mean: 0.264 - Std: 0.041\n",
      "5,0.05,500,BarzanMozafari,83,4.0-0.42,0.491,536.3000000000001,17.1,0.264,0.041,0.914,0.015,0.907,0.015\n",
      "  ---  \n",
      "RTE\n",
      "Results for cost: 500:\n",
      "Cost mean: 3.743 - Std: 0.042\n",
      "Recall mean: 0.98 - Std: 0.006\n",
      "Precision mean: 0.981 - Std: 0.006\n",
      "Loss mean: 0.061 - Std: 0.012\n",
      "5,0.05,500,RTE,164,4.0-0.15,0.5,149.72,1.6800000000000002,0.061,0.012,0.98,0.006,0.981,0.006\n",
      "  ---  \n",
      "SpamCF\n",
      "Results for cost: 500:\n",
      "Cost mean: 10.355 - Std: 0.274\n",
      "Recall mean: 0.92 - Std: 0.025\n",
      "Precision mean: 0.974 - Std: 0.019\n",
      "Loss mean: 0.3 - Std: 0.079\n",
      "5,0.05,500,SpamCF,150,4.0-0.4,0.3069306930693069,52.292750000000005,1.3837000000000002,0.3,0.079,0.92,0.025,0.974,0.019\n",
      "  ---  \n",
      "TEMP\n",
      "Results for cost: 500:\n",
      "Cost mean: 3.602 - Std: 0.051\n",
      "Recall mean: 0.978 - Std: 0.005\n",
      "Precision mean: 0.985 - Std: 0.002\n",
      "Loss mean: 0.07 - Std: 0.019\n",
      "5,0.05,500,TEMP,76,4.0-0.14,0.4393939393939394,83.20620000000001,1.1781,0.07,0.019,0.978,0.005,0.985,0.002\n",
      "  ---  \n",
      "WVSCM\n",
      "Results for cost: 500:\n",
      "Cost mean: 13.288 - Std: 0.332\n",
      "Recall mean: 0.792 - Std: 0.032\n",
      "Precision mean: 0.873 - Std: 0.029\n",
      "Loss mean: 0.703 - Std: 0.122\n",
      "5,0.05,500,WVSCM,17,4.0-0.5,0.3625,106.304,2.6560000000000006,0.703,0.122,0.792,0.032,0.873,0.029\n",
      "  ---  \n"
     ]
    }
   ],
   "source": [
    "import algorithms_utils as alg_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def round_to_3(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev = 1000,4.000000,0.300000,1000,4.000000,0.300000\n",
    "#name = \"%d,%.2f,%.2f,%d,%.2f,%.2f\" % (WP0_num,WP0_mean,WP0_stddev,WP1_num,WP1_mean,WP1_stddev)\n",
    "\n",
    "datasets = ['BarzanMozafari', 'RTE', 'SpamCF', 'TEMP', 'WVSCM']\n",
    "RELATIVEPATH = \"Experiments/Results/RealWorld/\"\n",
    "\n",
    "NUMBER_OF_REPETITIONS = 6\n",
    "loss_ratio = 5\n",
    "total_cost_mean = 0.\n",
    "initial_line = 8\n",
    "wrong_answer_costs = [500]\n",
    "\n",
    "gammas_dist = [(4.,0.42),(4.,0.15),(4.,0.4),(4.,0.14),(4.,0.5)]\n",
    "\n",
    "for key,name in enumerate(datasets):\n",
    "    print(name)\n",
    "\n",
    "    for wrongAnswerCost in wrong_answer_costs:\n",
    "        ground_truth, workers_accuracy = alg_utils.get_real_dataset_data(name)\n",
    "        NUMBER_OF_ITEMS = len(ground_truth)\n",
    "        \n",
    "        data_bal = sum(ground_truth) / NUMBER_OF_ITEMS\n",
    "        worker_num = len(workers_accuracy)\n",
    "        \n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "\n",
    "        for iteration in range(NUMBER_OF_REPETITIONS):\n",
    "            iteration_results_file = open(RELATIVEPATH + \"%s/%d/Unstarred/%d\" % (name,wrongAnswerCost,iteration),'r')\n",
    "            gt_vals = []\n",
    "            classification_vals = []\n",
    "            iteration_cost = 0.\n",
    "            for i, line in enumerate(iteration_results_file):\n",
    "                if (initial_line - 1 < i < (initial_line + NUMBER_OF_ITEMS - 1)):\n",
    "                    item_id, y_val, gt_val, item_cost, item_difficulty, item_real_difficulty,  = line.rstrip().split(\",\")\n",
    "                    classification_vals.append(int(y_val))\n",
    "                    gt_vals.append(int(gt_val))\n",
    "                    iteration_cost += float(item_cost)\n",
    "            # end for results file\n",
    "\n",
    "            costs.append(iteration_cost / NUMBER_OF_ITEMS)    \n",
    "            loss, recall, precision = alg_utils.Metrics.compute_metrics(classification_vals, gt_vals, loss_ratio)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "        # end for iterations\n",
    "\n",
    "        print(f\"Results for cost: {wrongAnswerCost}:\")\n",
    "        print(f\"Cost mean: {round_to_3(np.mean(costs))} - Std: {round_to_3(np.std(costs))}\")\n",
    "        print(f\"Recall mean: {round_to_3(np.mean(recalls))} - Std: {round_to_3(np.std(recalls))}\")\n",
    "        print(f\"Precision mean: {round_to_3(np.mean(precisions))} - Std: {round_to_3(np.std(precisions))}\")\n",
    "        print(f\"Loss mean: {round_to_3(np.mean(losses))} - Std: {round_to_3(np.std(losses))}\")\n",
    "        \n",
    "        norm_cost = round_to_3(np.mean(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        norm_cost_std = round_to_3(np.std(costs)) * NUMBER_OF_ITEMS * 0.05\n",
    "        \n",
    "        shape,scale = gammas_dist[key]\n",
    "        norm_gamma = f\"{shape}-{scale}\"\n",
    "        \n",
    "        #loss_ratio,cost_ratio,wrong_answer_cost,name,workers_num,gamma_dist,data_bal,cost,cost_std,loss,loss_std,recall,recall_std,precision,precision_std\n",
    "        print(f\"5,0.05,500,{name},{worker_num},{norm_gamma},{data_bal},{norm_cost},{norm_cost_std},{round_to_3(np.mean(losses))},{round_to_3(np.std(losses))},{round_to_3(np.mean(recalls))},{round_to_3(np.std(recalls))},{round_to_3(np.mean(precisions))},{round_to_3(np.std(precisions))}\")\n",
    "        \n",
    "        print(\"  ---  \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test compute better values for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: 10\n",
      "[-3.4997499999999997, -5.000000000000002, -5.000000000000002]\n",
      "Strategy: 1000\n",
      "[-88.09397909090907, -499.9999999999999, -409.090909090909]\n"
     ]
    }
   ],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "def myfindBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def myfindBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def findValues(actions, policy, beliefState):\n",
    "    bestValue = [-1230981239102938019] * len(actions)\n",
    "\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue[action]:\n",
    "                bestValue[action] = value\n",
    "\n",
    "    return bestValue\n",
    "\n",
    "numberOfWorkerPools = 1\n",
    "numStates = 23\n",
    "numberOfProblems = 100\n",
    "actions = range(0, numberOfWorkerPools+2)\n",
    "\n",
    "belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "#policies = {'frtdp':'/Users/pmaglione/Documents/pomdp_solve_test/frtdp.policy',\n",
    "#            'hsvi':'/Users/pmaglione/Documents/pomdp_solve_test/hsvi.policy'}\n",
    "\n",
    "policies = {'10': '/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/Policies/W1_COST-10.policy',\n",
    "           '1000': '/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/Policies/W1_COST-1000.policy'}\n",
    "\n",
    "values = {'frtdp':0, 'hsvi':0}\n",
    "\n",
    "for key,path in policies.items():\n",
    "    policy = readPolicy(path, numStates)\n",
    "    beliefState = beliefs[0]\n",
    "    #print(beliefState)\n",
    "    print(f\"Strategy: {key}\")\n",
    "    key_vals = findValues(actions, policy, beliefState)\n",
    "    values[key] = key_vals\n",
    "    print(key_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.0, 0.0060123097415043, 0.011839454877812516, 0.01746500369237076, 0.022868532129299814, 0.028023827666600165, 0.03289575883800424, 0.03743421992492823, 0.04156041989520074, 0.04512592461901333, 0.04761904761904763, 0.09523809523809526, 0.08922578549659095, 0.08339864036028273, 0.07777309154572448, 0.07236956310879543, 0.06721426757149508, 0.062342336400091014, 0.057803875313167025, 0.05367767534289451, 0.05011217061908192, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 0.01649468179411477, 0.030360078668685404, 0.04176478596555842, 0.05088695399564624, 0.05791634407769344, 0.06305727498908088, 0.06653313053722262, 0.06859395455811654, 0.06953149123097968, 0.06972260836580384, 0.0, 0.01649468179411477, 0.0303600786686854, 0.04176478596555841, 0.05088695399564624, 0.05791634407769343, 0.06305727498908088, 0.06653313053722262, 0.06859395455811655, 0.06953149123097971, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 0.002082593856709167, 0.007548382411179521, 0.015317864963090597, 0.02443790879064439, 0.0340837905557688, 0.04356065513147295, 0.05230293265731135, 0.059866664627860486, 0.0658911294706723, 0.06972260836580385, 0.0, 0.03090676973152038, 0.05317177492619129, 0.06821170696802624, 0.0773359992006481, 0.08174889759961808, 0.08255389484668885, 0.08076332841713392, 0.07732124448837263, 0.07317185299128712, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 0.0002265181001784914, 0.0016167489285725228, 0.004839760821152609, 0.010110189828678959, 0.017279545798561873, 0.025923360118961934, 0.03542026526420334, 0.045011342947700345, 0.05379105309524579, 0.06006359358800911, 0.0, 0.049888561039555475, 0.08022266945726846, 0.09597215830188163, 0.1012499070147772, 0.09940319793705157, 0.09310599019633115, 0.08445554045939495, 0.07508436561750167, 0.06633523148497304, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 2.239488903299609e-05, 0.0003147595722282626, 0.0013899437954495316, 0.0038019137228969928, 0.007962774814838843, 0.014022836165366362, 0.02180345183054241, 0.03076143245690503, 0.03991543055916958, 0.047032349616324874, 0.0, 0.07319746073933085, 0.11001723797986342, 0.12273806789418551, 0.1204911756218611, 0.10986680241365727, 0.09544767533261858, 0.08027674577682739, 0.06627471563371468, 0.054662831185186285, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 2.065768681005181e-06, 5.717446120756295e-05, 0.00037244088381410245, 0.00133392672605252, 0.0034236023461935885, 0.007077293341129458, 0.012522339377522363, 0.019614530152685373, 0.027634929400207765, 0.03436123845296223, 0.0, 0.10020233060386587, 0.14077033224323385, 0.14645360717387937, 0.13378352048795594, 0.11329725374791633, 0.09129349998556828, 0.0711931390107322, 0.0545799331984017, 0.04202684263799044, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 1.8067075434531701e-07, 9.846879196677587e-06, 9.46217232081955e-05, 0.00044374664841531684, 0.0013956470165557755, 0.0033866625743246555, 0.006818974711457979, 0.011858306157802525, 0.018140501771591228, 0.02380204852291649, 0.0, 0.13005678840741786, 0.1707791818511551, 0.16568922186673973, 0.14083914942732859, 0.11077599920875149, 0.08279188184332949, 0.059863198866303216, 0.042617848544285956, 0.030636193308465377, 0.0, 0.0]\n",
      "---\n",
      "0\n",
      "[0.0, 1.5154403389698264e-08, 1.6264471497845838e-06, 2.3055214857314203e-05, 0.00014157384078706314, 0.0005456481464644682, 0.001554252149634665, 0.0035612093623252393, 0.006875625673814934, 0.011420498163934086, 0.015812650812367084, 0.0, 0.16189485401523396, 0.19870259468279436, 0.17977664044686678, 0.14219652509042252, 0.1038763750480944, 0.07200795338216914, 0.04827547046837687, 0.031915000829314914, 0.021418431070989036, 0.0, 0.0]\n",
      "---\n",
      "2\n",
      "[0.0, 1.2296814872806275e-09, 2.5988673166314186e-07, 5.434383721470732e-06, 4.369520171614932e-05, 0.0002063728600196781, 0.0006900393077923512, 0.0017991970856211076, 0.003856599994376543, 0.006955423957594172, 0.010162436171283847, 0.0, 0.19495566951713672, 0.2236531162014806, 0.1887013765173913, 0.13888565116098908, 0.09423032766139722, 0.06058651704678333, 0.03766135276410033, 0.023120701369377227, 0.014485827682805543, 0.0, 0.0]\n",
      "---\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "numStates = 22\n",
    "numberOfProblems = 10\n",
    "belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "\n",
    "gammas = [0.628980,\n",
    "0.580959,\n",
    "1.281037,\n",
    "0.987592,\n",
    "1.373258,\n",
    "0.801062,\n",
    "1.488326,\n",
    "1.523445,\n",
    "1.448263,\n",
    "1.126245,\n",
    "1.046380,\n",
    "1.418695,\n",
    "1.118415,\n",
    "1.142464,\n",
    "1.032052,\n",
    "0.860525,\n",
    "1.418732]\n",
    "\n",
    "gammas = [1.281037] * 100\n",
    "\n",
    "difficulties = getDifficulties(0.1)\n",
    "gen_belief = beliefs[0]\n",
    "observation = 1\n",
    "\n",
    "num = 0\n",
    "\n",
    "for gamma in gammas:\n",
    "    gen_belief = updateBelief(gen_belief, observation, difficulties, gamma)\n",
    "    action = findBestAction([0,1,2], policy, gen_belief)\n",
    "    print(action)\n",
    "    print(gen_belief)\n",
    "    #print(gen_belief.index(max(gen_belief)))\n",
    "    print(\"---\")\n",
    "    num = num + 1\n",
    "    if (action != 0):\n",
    "        print(num)\n",
    "        break\n",
    "    observation = np.random.binomial(1, 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-63.626511363636375\n",
      "-249.49999999999994\n",
      "-249.50000000000009\n"
     ]
    }
   ],
   "source": [
    "b = [0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    " 0.045454545454545456,\n",
    "0.]\n",
    "p0 = [-3.9995, -4.97644, -6.20159, -7.90205, -10.5238, -15.1055, -24.4401, -46.3647, -98.0642, -192.416, -291.557, -3.9995, -4.97644, -6.20159, -7.90204, -10.5233, -15.0967, -24.3655, -46.0182, -97.2571, -191.371, -290.521, 0.0]\n",
    "p1 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, 0.0]\n",
    "p2 = [-500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0]\n",
    "\n",
    "print(dot(b, p0))\n",
    "print(dot(b, p1))\n",
    "print(dot(b, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeliefState: [0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-d90d6c3d8ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/Policies/W1_COST-10.policy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyReadPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumStates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Strategy: {method}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-d90d6c3d8ded>\u001b[0m in \u001b[0;36mmyReadPolicy\u001b[0;34m(path, numStates)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0malpha_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malpha_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "def myfindBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def myfindBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def findValues(actions, policy, beliefState):\n",
    "    bestValue = [-1230981239102938019] * len(actions)\n",
    "\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = myfindBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue[action]:\n",
    "                bestValue[action] = value\n",
    "\n",
    "    return bestValue\n",
    "\n",
    "numberOfWorkerPools = 1\n",
    "numStates = 23\n",
    "numberOfProblems = 100\n",
    "actions = range(0, numberOfWorkerPools+2)\n",
    "\n",
    "belief = [1 for i in range(numStates)]  # init , equivalent to [1] * numStates\n",
    "belief[numStates-1] = 0  # last states = 0, terminating state\n",
    "belief = normalize(belief)\n",
    "beliefs = [deepcopy(belief) for i in range(numberOfProblems)]\n",
    "\n",
    "def myReadPolicy(path, numStates):\n",
    "    f = open(path, \"r\")\n",
    "    alpha_vectors = {0: [], 1:[], 2:[]}\n",
    "\n",
    "    action = -1\n",
    "    for line in f:\n",
    "        if len(line) == 2:\n",
    "            action = int(line)\n",
    "        elif len(line) > 2:\n",
    "            alpha_vectors[action].append([float(x) for x in line.split()])\n",
    "    \n",
    "    return alpha_vectors\n",
    "    \n",
    "\n",
    "path = '/Users/pmaglione/Documents/pomdp_solve_test/pomdp-solve'\n",
    "values = {}\n",
    "\n",
    "methods = ['enum','twopass','witness','incprune']\n",
    "beliefState = beliefs[0]\n",
    "print(f\"BeliefState: {beliefState}\")\n",
    "    \n",
    "#for method in methods:\n",
    "file = f\"/Users/pmaglione/Repos/adaptive-pomdp-solutions/WorkerPoolSelection/ModelLearning/Policies/W1_COST-10.policy\"\n",
    "\n",
    "policy = myReadPolicy(file, numStates)\n",
    "\n",
    "print(f\"Strategy: {method}\")\n",
    "key_vals = findValues(actions, policy, beliefState)\n",
    "values[key] = key_vals\n",
    "print(key_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 states POMDP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelLearning.utils import *\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from truth_finder import expectation_maximization\n",
    "\n",
    "#metrics\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    #k penalization for false negatives\n",
    "    def compute_metrics(items_classification, gt, lr = 1):\n",
    "        # FP == False Inclusion\n",
    "        # FN == False Exclusion\n",
    "        fp = fn = tp = tn = 0.\n",
    "        for i in range(len(gt)):\n",
    "            gt_val = gt[i]\n",
    "            cl_val = items_classification[i]\n",
    "\n",
    "            if gt_val and not cl_val:\n",
    "                fn += 1\n",
    "            if not gt_val and cl_val:\n",
    "                fp += 1\n",
    "            if gt_val and cl_val:\n",
    "                tp += 1\n",
    "            if not gt_val and not cl_val:\n",
    "                tn += 1\n",
    "                        \n",
    "\n",
    "        recall = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        loss = (fp + (fn * lr)) / len(gt)\n",
    "        \n",
    "        return loss, recall, precision\n",
    "#end\n",
    "\n",
    "def simulate_workers(workers_num, cheaters_prop, fixed_acc, workers_acc, base_acc = .5):\n",
    "    workers = {}\n",
    "    for i in range(workers_num):\n",
    "        if (fixed_acc == False):\n",
    "            if np.random.binomial(1, cheaters_prop):\n",
    "                # worker_type is 'rand_ch'\n",
    "                worker_acc_pos = worker_acc_neg = 0.5\n",
    "            else:\n",
    "                # worker_type is 'worker'\n",
    "                worker_acc_pos = base_acc + (np.random.beta(1, 1) * (1 - base_acc))\n",
    "                #worker_acc_neg = worker_acc_pos + 0.1 if worker_acc_pos + 0.1 <= 1. else 1.\n",
    "                worker_acc_neg = worker_acc_pos\n",
    "        else:\n",
    "            worker_acc_pos = workers_acc\n",
    "            worker_acc_neg = worker_acc_pos\n",
    "\n",
    "        workers[i] = [worker_acc_pos, worker_acc_neg]\n",
    "\n",
    "    return workers\n",
    "\n",
    "def get_random_worker_accuracy(workers_accuracy, item_id, votes):\n",
    "    item_votes = votes[item_id].copy()\n",
    "    worker_ids_used = item_votes.keys()\n",
    "    workers_ids_range = workers_accuracy.keys()\n",
    "    workers_ids_unused = [val for val in workers_ids_range if val not in worker_ids_used]\n",
    "    \n",
    "    if (len(workers_ids_unused) == 0):\n",
    "        used = len(worker_ids_used)\n",
    "        ranges = len(workers_ids_range)\n",
    "        unu = len(workers_ids_unused)\n",
    "        print(f'used: {used}')\n",
    "        print(f'workers: {ranges}')\n",
    "        print(f'unused: {unu}')\n",
    "        raise ValueError(\"Unused empty!?\")\n",
    "    \n",
    "    selected_worker_id = np.random.choice(workers_ids_unused)\n",
    "    worker_acc_pos = workers_accuracy[selected_worker_id][0]\n",
    "    worker_acc_neg = workers_accuracy[selected_worker_id][1]\n",
    "\n",
    "    return {'worker_id': selected_worker_id, 'acc_pos':worker_acc_pos, 'acc_neg': worker_acc_neg}\n",
    "\n",
    "\n",
    "def get_worker_vote(workers_accuracy, i, gt, votes):\n",
    "    worker_data = get_random_worker_accuracy(workers_accuracy, i, votes)\n",
    "    worker_id, worker_acc_pos, worker_acc_neg = worker_data['worker_id'], worker_data['acc_pos'], worker_data['acc_neg']\n",
    "\n",
    "    if (gt[i]):\n",
    "        worker_acc = worker_acc_pos\n",
    "    else:\n",
    "        worker_acc = worker_acc_neg\n",
    "\n",
    "    if np.random.binomial(1, worker_acc):\n",
    "        vote = gt[i]\n",
    "    else:\n",
    "        vote = 1 - gt[i]\n",
    "\n",
    "    return (worker_id, vote)\n",
    "\n",
    "def generate_gold_data(items_num, possitive_percentage):\n",
    "    pos_items_number = int(round(((possitive_percentage * 100) * items_num) / 100))     \n",
    "    gold_data = ([1] * pos_items_number) + ([0] * (items_num - pos_items_number))\n",
    "    random.shuffle(gold_data)\n",
    "\n",
    "    return gold_data\n",
    "\n",
    "def findBestValue(action, hyperplanes, beliefs):\n",
    "    bestValue = -129837198273981231\n",
    "    bestHyperplane = []\n",
    "    amount = 0\n",
    "\n",
    "    for hyperplane in hyperplanes:\n",
    "        dontUse = False\n",
    "        for (b, entry) in zip(beliefs, hyperplane):\n",
    "            if b != 0 and entry == '*':\n",
    "                dontUse = True\n",
    "                break\n",
    "        if dontUse:\n",
    "            amount = amount + 1\n",
    "            continue\n",
    "        \n",
    "        value = dot(beliefs, hyperplane)\n",
    "        if value > bestValue:\n",
    "            bestHyperplane = hyperplane\n",
    "            bestValue = value\n",
    "     \n",
    "    return bestValue\n",
    "\n",
    "def findBestAction(actions, policy, beliefState):\n",
    "    bestValue = -1230981239102938019\n",
    "    bestAction = 0  #Assume there is at least one action\n",
    "    for action in actions:\n",
    "        if action in policy:\n",
    "            value = findBestValue(action, policy[action], beliefState)\n",
    "            if value > bestValue:\n",
    "                bestValue = value\n",
    "                bestAction = action\n",
    "    return bestAction\n",
    "\n",
    "def myReadPolicy(path, states, origin):\n",
    "    if origin != 'zmdp':\n",
    "        f = open(path, \"r\")\n",
    "        alpha_vectors = {0: [], 1:[], 2:[]}\n",
    "\n",
    "        action = -1\n",
    "        for line in f:\n",
    "            if len(line) == 2:\n",
    "                action = int(line)\n",
    "            elif len(line) > 2:\n",
    "                strategy_vectors = [float(x) for x in line.split()]\n",
    "                if len(strategy_vectors) == states:\n",
    "                    alpha_vectors[action].append(strategy_vectors)\n",
    "    else:\n",
    "        alpha_vectors = readPolicy(path, states)\n",
    "    \n",
    "    return alpha_vectors\n",
    "\n",
    "def update_belief(prevBelief, vote, acc, states_per_class):\n",
    "    newBeliefs = []\n",
    "    for i in range(0, 2):  # 0,1\n",
    "        for j in range(0, states_per_class):\n",
    "            state = (i * states_per_class) + j\n",
    "            \n",
    "            if vote == i:\n",
    "                newBeliefs.append(acc * prevBelief[state])\n",
    "            else:\n",
    "                newBeliefs.append((1 - acc) * prevBelief[state])\n",
    "                \n",
    "    newBeliefs.append(0.0)\n",
    "    normalize(newBeliefs)\n",
    "    return newBeliefs\n",
    "\n",
    "def input_adapter(responses):\n",
    "    '''\n",
    "    :param responses:\n",
    "    :return: Psi\n",
    "    '''\n",
    "    Psi = [[] for _ in responses.keys()]\n",
    "    i = 0\n",
    "    for obj_id, obj_responses in responses.items():\n",
    "        k = 0\n",
    "        for worker_id, worker_response in obj_responses.items():\n",
    "            Psi[i].append((k, worker_response[0]))\n",
    "            k += 1\n",
    "        i += 1\n",
    "    return Psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(policy, items, states, num_actions, workers_accuracy, ground_truth, acc):\n",
    "    belief = [1 for i in range(states)]  # init , equivalent to [1] * numStates\n",
    "    belief[states-1] = 0  # last states = 0, terminating state\n",
    "    belief = normalize(belief)\n",
    "    beliefs = [deepcopy(belief) for i in range(items)]\n",
    "    beliefState = beliefs[0]\n",
    "    actions = range(0, num_actions)\n",
    "    items_classification = {}\n",
    "    votes = {i:{} for i in range(items)}\n",
    "\n",
    "    states_per_class = int((states - 1) / 2)\n",
    "\n",
    "    while len(items_classification) != items:\n",
    "        for item_id, belief_state in enumerate(beliefs):\n",
    "            if item_id not in items_classification.keys(): # if item is classified\n",
    "                best_action = findBestAction(actions, policy, belief_state)\n",
    "\n",
    "                #prevent\n",
    "                if len(votes[item_id]) > 100:\n",
    "                    mv = (sum(votes[item_id]) / len(votes[item_id])) > 0.5\n",
    "                    if mv:\n",
    "                        best_action = 2\n",
    "                    else:\n",
    "                        best_action = 1\n",
    "\n",
    "                if best_action == 0:\n",
    "                    worker_id, vote = get_worker_vote(workers_accuracy, item_id, ground_truth, votes)\n",
    "                    votes[item_id][worker_id] = [vote]\n",
    "                    \n",
    "                    belief_state = update_belief(belief_state, vote, acc, states_per_class)\n",
    "                    \n",
    "                    beliefs[item_id] = belief_state\n",
    "                elif best_action == 1: #submit zero\n",
    "                    items_classification[item_id] = 0\n",
    "                else: #submit one\n",
    "                    items_classification[item_id] = 1\n",
    "                    \n",
    "        #end for\n",
    "        if min([len(v) for a,v in votes.items()]) == 3:\n",
    "            accs, p = expectation_maximization(max([len(v) for a,v in votes.items()]), items, input_adapter(votes))\n",
    "            acc = np.mean(accs)\n",
    "        \n",
    "    #end while\n",
    "    num_votes = sum([len(v) for k,v in votes.items()])\n",
    "\n",
    "    return items_classification, num_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true_percentage = .5\n",
    "states = 3\n",
    "items = 100\n",
    "num_actions = 3\n",
    "lr = 5\n",
    "acc = .75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc):\n",
    "    ground_truth = generate_gold_data(items, data_true_percentage)\n",
    "    workers_accuracy = simulate_workers(1000, 0, False, acc, base_acc = .5)\n",
    "    \n",
    "    losses  = []\n",
    "    recalls  = []\n",
    "    precisions  = []\n",
    "    votes_amount = []\n",
    "    for _ in range(50):\n",
    "        items_classification, num_votes = run_exp(policy, items, states, num_actions, workers_accuracy, ground_truth, acc)\n",
    "        \n",
    "        votes_amount.append(num_votes / items) #average\n",
    "        \n",
    "        loss,  recall, precision = Metrics.compute_metrics(items_classification, ground_truth, lr)\n",
    "        losses.append(loss)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    print(f\" Votes: {np.mean(votes_amount)} / {np.std(votes_amount)} \\n Loss: {np.mean(losses)} / {np.std(losses)} \\n Recall: {np.mean(recalls)} / {np.std(recalls)} \\n Precision: {np.mean(precisions)} / {np.std(precisions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Votes: 9.6322 / 0.9893680609358683 \n",
      " Loss: 0.0238 / 0.03205557673790942 \n",
      " Recall: 0.992 / 0.01200000000000001 \n",
      " Precision: 0.9925161202942715 / 0.012344492992408751\n",
      " --- \n",
      " Votes: 10.5534 / 1.190007747873937 \n",
      " Loss: 0.0068000000000000005 / 0.014891608375189026 \n",
      " Recall: 0.9980000000000001 / 0.006000000000000005 \n",
      " Precision: 0.9964705882352942 / 0.007533087338156308\n",
      " --- \n",
      " Votes: 1.0 / 0.0 \n",
      " Loss: 0.7298 / 0.16922753913001276 \n",
      " Recall: 0.7568 / 0.06564876236457166 \n",
      " Precision: 0.7577381369027768 / 0.04809981928535244\n",
      " --- \n"
     ]
    }
   ],
   "source": [
    "policies = []\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-500/test.alpha', states, 'pomdp-solve'))\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-1000/test.alpha', states, 'pomdp-solve'))\n",
    "policies.append(myReadPolicy('/Users/pmaglione/Documents/pomdp_solve_test/3states/-10/test.alpha', states, 'pomdp-solve'))\n",
    "\n",
    "for policy in policies:\n",
    "    run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc)\n",
    "    print(\" --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Votes: 3.1920000000000006 / 0.17577258034175863 \n",
      " Loss: 0.2926 / 0.08666741025322032 \n",
      " Recall: 0.9036 / 0.033392214661504556 \n",
      " Precision: 0.8992675825856574 / 0.040620210608958304\n"
     ]
    }
   ],
   "source": [
    "states = 23\n",
    "policy = myReadPolicy('./ModelLearning/Policies/W1_COST500.policy', states, 'zmdp')\n",
    "run_experiment(policy, data_true_percentage, states, items, num_actions, lr, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O: 3: 0 : None 1.00000\n",
      "O: 3: 1 : None 1.00000\n",
      "O: 3: 2 : None 1.00000\n",
      "O: 3: 3 : None 1.00000\n",
      "O: 3: 4 : None 1.00000\n",
      "O: 3: 5 : None 1.00000\n",
      "O: 3: 6 : None 1.00000\n",
      "O: 3: 7 : None 1.00000\n",
      "O: 3: 8 : None 1.00000\n",
      "O: 3: 9 : None 1.00000\n",
      "O: 3: 10 : None 1.00000\n",
      "O: 3: 11 : None 1.00000\n",
      "O: 3: 12 : None 1.00000\n",
      "O: 3: 13 : None 1.00000\n",
      "O: 3: 14 : None 1.00000\n",
      "O: 3: 15 : None 1.00000\n",
      "O: 3: 16 : None 1.00000\n",
      "O: 3: 17 : None 1.00000\n",
      "O: 3: 18 : None 1.00000\n",
      "O: 3: 19 : None 1.00000\n",
      "O: 3: 20 : None 1.00000\n",
      "O: 3: 21 : None 1.00000\n"
     ]
    }
   ],
   "source": [
    "for i in range(22):\n",
    "    '''\n",
    "    print(f\"T: 0 : {i} : {i} 1.000000\")\n",
    "    print(f\"T: 1 : {i} : 22 1.000000\")  \n",
    "    print(f\"T: 2 : {i} : 22 1.000000\")\n",
    "    print(f\"T: 3 : {i} : 23 1.000000\")\n",
    "    '''\n",
    "    print(f\"O: 3: {i} : None 1.00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"T: 3 : {i} : 23 1.000000\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
